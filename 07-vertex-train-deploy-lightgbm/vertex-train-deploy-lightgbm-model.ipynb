{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:migration,new"
   },
   "source": [
    "# Vertex AI: Train and deploy LightGBM models with custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:iris,lcn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Iris dataset](https://www.tensorflow.org/datasets/catalog/iris) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This dataset does not require any feature engineering. The version of the dataset you will use in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_local"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "If you are using Colab or Google Cloud Notebooks, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
    "\n",
    "- The Cloud Storage SDK\n",
    "- Git\n",
    "- Python 3\n",
    "- virtualenv\n",
    "- Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Cloud Storage guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
    "\n",
    "1. [Install and initialize the SDK](https://cloud.google.com/sdk/docs/).\n",
    "\n",
    "2. [Install Python 3](https://cloud.google.com/python/setup#installing_python).\n",
    "\n",
    "3. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3.  Activate the virtual environment.\n",
    "\n",
    "4. To install Jupyter, run `pip3 install jupyter` on the command-line in a terminal shell.\n",
    "\n",
    "5. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "6. Open this notebook in the Jupyter Notebook Dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_storage"
   },
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_storage"
   },
   "outputs": [],
   "source": [
    "!pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install latest version of [`ligthgbm`](https://pypi.org/project/lightgbm/) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U lightgbm $USER_FLAG\n",
    "#!conda install -c conda-forge -y lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tensorflow"
   },
   "outputs": [],
   "source": [
    "if os.environ[\"IS_TESTING\"]:\n",
    "    ! pip3 install --upgrade tensorflow $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### GPU runtime\n",
    "\n",
    "This tutorial does not require a GPU runtime.\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. If you are running this notebook locally, you will need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "**Click Create service account**.\n",
    "\n",
    "In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "\n",
    "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcp_authenticate"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Google Cloud Notebook, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LightGBM version {lightgbm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"iris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction,xgboost"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for training and prediction.\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction,xgboost"
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"lightgbm-cpu\"\n",
    "DEPLOY_VERSION = \"lightgbm-cpu\"\n",
    "\n",
    "TRAIN_IMAGE = f\"gcr.io/{PROJECT_ID}/training/{TRAIN_VERSION}:latest\"\n",
    "DEPLOY_IMAGE = f\"gcr.io/{PROJECT_ID}/prediction/{DEPLOY_VERSION}:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training,prediction"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "### Examine the training package\n",
    "\n",
    "![training-with-custom-containers-on-vertex-training](./images/training-with-custom-containers-on-vertex-training.png)\n",
    "\n",
    "#### Package layout\n",
    "\n",
    "Before you start the training, you will look at how a custom container is created for a custom training job. The training application code is contains the following directory/file layout.\n",
    "\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job. *Note*, when we referred to it in the worker pool specification, we replace the directory slash with a dot (`trainer.task`) and dropped the file suffix (`.py`).\n",
    "\n",
    "#### Building custom training container\n",
    "\n",
    "In the following cells, you will build a custom training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "examine_training_package"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taskpy_contents:xgboost,iris"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single Instance Training for Iris\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import argparse\n",
    "\n",
    "print(\"Parsing arguments\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--model-dir', \n",
    "    dest='model_dir',        \n",
    "    default=os.getenv('AIP_MODEL_DIR'), \n",
    "    type=str, \n",
    "    help='Location to export GCS model')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading data\")\n",
    "iris = load_iris()\n",
    "print(iris.data.shape)\n",
    "\n",
    "# split data\n",
    "print(\"Splitting data into test and train\")\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "print(\"creating dataset for LightGBM\")\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'metric': {'multi_error'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'num_class' : 3\n",
    "}\n",
    "\n",
    "# train lightgbm model\n",
    "print('Starting training...')\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "# save model to file\n",
    "print('Saving model...')\n",
    "model_filename = 'model.txt'\n",
    "gbm.save_model(model_filename)\n",
    "\n",
    "# Upload the saved model file to Cloud Storage\n",
    "gcs_model_path = os.path.join(args.model_dir, model_filename)\n",
    "print(f\"Copying model files to {gcs_model_path}\")\n",
    "subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "    stderr=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and push custom container with training script\n",
    "\n",
    "Next, you package the training folder into a container, and then push it to Container Registry or Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./custom/Dockerfile\n",
    "\n",
    "FROM gcr.io/cloud-aiplatform/training/scikit-learn-cpu.0-23\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip3 install -U lightgbm\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./trainer/ ./trainer/\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --tag {TRAIN_IMAGE} ./custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_a_model:migration"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_create:migration,new,mbsdk,prebuilt"
   },
   "source": [
    "### [Training with custom containers](https://cloud.google.com/vertex-ai/docs/training/containers-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomContainerTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The custom training container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = f\"{APP_NAME}-lightgbm-cstm-cntr-{TIMESTAMP}\"\n",
    "\n",
    "print(f\"APP_NAME={APP_NAME}\")\n",
    "print(f\"CUSTOM_TRAIN_IMAGE_URI={TRAIN_IMAGE}\")\n",
    "print(f\"JOB_NAME={JOB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the job with container image spec\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=f\"{JOB_NAME}\", \n",
    "    container_uri=f\"{TRAIN_IMAGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(job))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model"
   },
   "source": [
    "*Example output:*\n",
    "\n",
    "    <google.cloud.aiplatform.training_jobs.CustomTrainingJob object at 0x7feab1346710>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model,cpu"
   },
   "source": [
    "#### Run the custom training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `replica_count`: The number of compute instances for training (replica_count = 1 is single node training).\n",
    "- `machine_type`: The machine type for the compute instances.\n",
    "- `base_output_dir`: The Cloud Storage location to write the model artifacts to.\n",
    "- `sync`: Whether to block until completion of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model,cpu"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = f\"{BUCKET_NAME}/{APP_NAME}-lightgbm-cstm-cntr/{TIMESTAMP}\"\n",
    "print(f\"Base output directory {MODEL_DIR}/\")\n",
    "\n",
    "model = job.run(\n",
    "    replica_count=1, \n",
    "    machine_type=TRAIN_COMPUTE, \n",
    "    base_output_dir=f\"{MODEL_DIR}/\", \n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_to_deploy = MODEL_DIR + '/model/'\n",
    "print(f\"Model path with trained model artifacts {model_path_to_deploy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -r $model_path_to_deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_model:migration,new"
   },
   "source": [
    "## Deploy LightGBM model to Vertex AI\n",
    "\n",
    "![serving-with-custom-containers-on-vertex-predictions](./images/serving-with-custom-containers-on-vertex-predictions.png)\n",
    "\n",
    "### Build a HTTP Server with FastAPI\n",
    "\n",
    "Custom Container image [requires](https://cloud.google.com/ai-platform-unified/docs/predictions/custom-container-requirements#image) that the container must run an HTTP server. Specifically, the container must listen and respond to liveness checks, health checks, and prediction requests.\n",
    "\n",
    "In this tutorial, we will use FastAPI to implement the HTTP server. The HTTP server must listen for requests on 0.0.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf serve\n",
    "! mkdir serve\n",
    "\n",
    "# Make the predictor subfolder\n",
    "! mkdir serve/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve/app/main.py\n",
    "from fastapi.logger import logger\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import lightgbm as lgb\n",
    "\n",
    "import logging\n",
    "\n",
    "gunicorn_logger = logging.getLogger('gunicorn.error')\n",
    "logger.handlers = gunicorn_logger.handlers\n",
    "\n",
    "if __name__ != \"main\":\n",
    "    logger.setLevel(gunicorn_logger.level)\n",
    "else:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model_f = \"/model/model.txt\"\n",
    "\n",
    "logger.info(\"Loading model\")\n",
    "_model = lgb.Booster(model_file=model_f)\n",
    "logger.info(\"Loading target class labels\")\n",
    "_class_names = load_iris().target_names\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    \"\"\" health check to ensure HTTP server is ready to handle \n",
    "        prediction requests\n",
    "    \"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "\n",
    "    instances = body[\"instances\"]\n",
    "    inputs = np.asarray(instances)\n",
    "    \n",
    "    outputs = _model.predict(inputs)\n",
    "\n",
    "    logger.info(f\"Outputs {outputs}\")\n",
    "    return {\"predictions\": [_class_names[class_num] for class_num in np.argmax(outputs, axis=1)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve/requirements.txt\n",
    "\n",
    "numpy~=1.20\n",
    "scikit-learn~=0.24\n",
    "pandas==1.0.4\n",
    "lightgbm==3.2.1\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add pre-start script\n",
    "FastAPI will execute this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on same the port expected by AI Platform (Unified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve/app/prestart.sh\n",
    "\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store test instances to use later\n",
    "To learn more about formatting input instances in JSON, [read the documentation.](https://cloud.google.com/ai-platform-unified/docs/predictions/online-predictions-custom-models#request-body-details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve/instances.json\n",
    "{\n",
    "    \"instances\": [\n",
    "        [6.7, 3.1, 4.7, 1.5],\n",
    "        [4.6, 3.1, 1.5, 0.2]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push prediction container to Container Registry\n",
    "\n",
    "Write the Dockerfile, using `tiangolo/uvicorn-gunicorn-fastapi` as a base image. This will automatically run FastAPI for you using Gunicorn and Uvicorn. Visit [the FastAPI docs to read more about deploying FastAPI with Docker](https://fastapi.tiangolo.com/deployment/docker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $MODEL_DIR \n",
    "\n",
    "MODEL_DIR=$1\n",
    "\n",
    "mkdir -p ./serve/model/\n",
    "gsutil cp -r ${MODEL_DIR}/model/ ./serve/model/ \n",
    "\n",
    "cat > ./serve/Dockerfile <<EOF\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY ./app /app\n",
    "COPY ./model /\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "EXPOSE 7080\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7080\"]\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIP_STORAGE_URI=f\"{MODEL_DIR}\"\n",
    "print(AIP_STORAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Build* the image and tag the Artifact Registry path that you will push to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --tag={DEPLOY_IMAGE} ./serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and test the container locally (optional)\n",
    "\n",
    "Run the container locally in detached mode and provide the environment variables that the container requires. These env vars will be provided to the container by AI Platform Prediction once deployed. Test the `/health` and `/predict` routes, then stop the running image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before push the container image to Container Registry to use it with Vertex Predictions, you can run it as a container in your local environment to verify that the server works as expected\n",
    "\n",
    "- To run the container image as a container locally, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker rm local-iris\n",
    "!docker run -t -d --rm -p 7080:7080 \\\n",
    "    --name=local-iris \\\n",
    "    -e AIP_HTTP_PORT=7080 \\\n",
    "    -e AIP_HEALTH_ROUTE=/health \\\n",
    "    -e AIP_PREDICT_ROUTE=/predict \\\n",
    "    -e AIP_STORAGE_URI={MODEL_DIR} \\\n",
    "    {DEPLOY_IMAGE}\n",
    "!docker container ls\n",
    "!sleep 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To send the container's server a health check, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:7080/health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, the server returns the following response:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"status\": \"healthy\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To send the container's server a prediction request, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -d @serve/instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  http://localhost:7080/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This request uses a test sentence. If successful, the server returns prediction in below format:\n",
    "\n",
    "```\n",
    "{\"predictions\":[\"versicolor\",\"setosa\"]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To stop the container, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the serving container to Vertex Predictions\n",
    "We create a model resource on Vertex AI and deploy the model to a Vertex Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions.\n",
    "\n",
    "#### Push the serving container to Container Registry\n",
    "Push your container image with inference code and dependencies to your Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model:mbsdk"
   },
   "source": [
    "## Upload the model\n",
    "\n",
    "Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource.\n",
    "- `artifact`: The Cloud Storage location of the trained model artifacts.\n",
    "- `serving_container_image_uri`: The serving container image.\n",
    "- `serving_container_predict_route`:  HTTP path to send prediction requests to the container.\n",
    "- `serving_container_health_route`: HTTP path to send health check requests to the container.\n",
    "- `serving_container_ports`: The ports exposed by the container to listen to requests.\n",
    "- `sync`: Whether to execute the upload asynchronously or synchronously.\n",
    "\n",
    "If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_display_name = f\"{APP_NAME}-{TIMESTAMP}\"\n",
    "model_description = \"LightGBM based iris flower classifier with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = \"/predict\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model:mbsdk"
   },
   "source": [
    "*Example output:*\n",
    "\n",
    "    INFO:google.cloud.aiplatform.models:Creating Model\n",
    "    INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/759209241365/locations/us-central1/models/925164267982815232/operations/3458372263047331840\n",
    "    INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/759209241365/locations/us-central1/models/925164267982815232\n",
    "    INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
    "    INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/759209241365/locations/us-central1/models/925164267982815232')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_predictions:migration"
   },
   "source": [
    "## Make batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batchpredictionjobs_create:migration,new,mbsdk"
   },
   "source": [
    "Documentation - [Batch prediction requests - Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_items:xgboost,tabular,iris"
   },
   "source": [
    "### Make test items\n",
    "\n",
    "You will use synthetic data as a test data items. Don't be concerned that we are using synthetic data -- we just want to demonstrate how to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_test_items:xgboost,tabular,iris"
   },
   "outputs": [],
   "source": [
    "INSTANCES = [[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_batch_file:custom,tabular,list"
   },
   "source": [
    "### Make the batch input file\n",
    "\n",
    "Now make a batch input file, which you will store in your local Cloud Storage bucket.  Each instance in the prediction request is a list of the form:\n",
    "\n",
    "                        [ [ content_1], [content_2] ]\n",
    "\n",
    "- `content`: The feature values of the test item as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[json.dumps(record) for record in INSTANCES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_batch_file:custom,tabular,list"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gcs_input_uri = f\"{BUCKET_NAME}/{APP_NAME}/test/batch_input/test.jsonl\"\n",
    "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "    for i in INSTANCES:\n",
    "        f.write(str(i) + \"\\n\")\n",
    "\n",
    "! gsutil cat $gcs_input_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,jsonl,custom,cpu"
   },
   "source": [
    "### Make the batch prediction request\n",
    "\n",
    "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
    "\n",
    "- `job_display_name`: The human readable name for the batch prediction job.\n",
    "- `gcs_source`: A list of one or more batch request input files.\n",
    "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
    "- `instances_format`: The format for the input instances, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request:mbsdk,jsonl,custom,cpu"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"{APP_NAME}_TIMESTAMP\",\n",
    "    gcs_source=gcs_input_uri,\n",
    "    gcs_destination_prefix=f\"{BUCKET_NAME}/{APP_NAME}/test/batch_output/\",\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    model_parameters=None,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "print(batch_predict_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request:mbsdk,jsonl,custom,cpu"
   },
   "source": [
    "*Example output:*\n",
    "\n",
    "    INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
    "    <google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7f806a6112d0> is waiting for upstream dependencies to complete.\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/759209241365/locations/us-central1/batchPredictionJobs/5110965452507447296\n",
    "    INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
    "    INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/759209241365/locations/us-central1/batchPredictionJobs/5110965452507447296')\n",
    "    INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
    "    https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/5110965452507447296?project=759209241365\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/5110965452507447296 current state:\n",
    "    JobState.JOB_STATE_RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "### Wait for completion of batch prediction job\n",
    "\n",
    "Next, wait for the batch job to complete. Alternatively, one can set the parameter `sync` to `True` in the `batch_predict()` method to block until the batch prediction job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "batch_predict_job.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_request_wait:mbsdk"
   },
   "source": [
    "*Example Output:*\n",
    "\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328\n",
    "    INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
    "    INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328')\n",
    "    INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
    "    https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/181835033978339328?project=759209241365\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_RUNNING\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328 current state:\n",
    "    JobState.JOB_STATE_SUCCEEDED\n",
    "    INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/759209241365/locations/us-central1/batchPredictionJobs/181835033978339328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lcn"
   },
   "source": [
    "### Get the predictions\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `instance`: The prediction request.\n",
    "- `prediction`: The prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lcn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            line = json.loads(line)\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lcn"
   },
   "source": [
    "*Example Output:*\n",
    "\n",
    "    {'instance': [6.7, 3.1, 4.7, 1.5], 'prediction': 'versicolor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_online_predictions:migration"
   },
   "source": [
    "## Make online predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:migration,new,mbsdk"
   },
   "source": [
    "Documentation - [Online prediction request](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,cpu"
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy_model:mbsdk,cpu"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = f\"{APP_NAME}-{TIMESTAMP}\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,cpu"
   },
   "source": [
    "*Example output:*\n",
    "\n",
    "    INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
    "    INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/759209241365/locations/us-central1/endpoints/4867177336350441472/operations/4087251132693348352\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/759209241365/locations/us-central1/endpoints/4867177336350441472\n",
    "    INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
    "    INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/759209241365/locations/us-central1/endpoints/4867177336350441472')\n",
    "    INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/759209241365/locations/us-central1/endpoints/4867177336350441472\n",
    "    INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/759209241365/locations/us-central1/endpoints/4867177336350441472/operations/1691336130932244480\n",
    "    INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/759209241365/locations/us-central1/endpoints/4867177336350441472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endpoints_predict:migration,new,mbsdk"
   },
   "source": [
    "### [predictions.online-prediction-automl](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-automl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_test_item:xgboost,tabular,iris"
   },
   "source": [
    "### Make test item\n",
    "\n",
    "You will use synthetic data as a test data item. Don't be concerned that we are using synthetic data -- we just want to demonstrate how to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_test_item:xgboost,tabular,iris"
   },
   "outputs": [],
   "source": [
    "INSTANCE = [1.4, 1.3, 5.1, 2.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "predict_request:mbsdk,custom,lcn"
   },
   "source": [
    "### Make the prediction\n",
    "\n",
    "Now that your `Model` resource is deployed to an `Endpoint` resource, you can do online predictions by sending prediction requests to the `Endpoint` resource.\n",
    "\n",
    "#### Request\n",
    "\n",
    "The format of each instance is:\n",
    "\n",
    "    [feature_list]\n",
    "\n",
    "Since the predict() method can take multiple items (instances), send your single test item as a list of one test item.\n",
    "\n",
    "#### Response\n",
    "\n",
    "The response from the predict() call is a Python dictionary with the following entries:\n",
    "\n",
    "- `ids`: The internal assigned unique identifiers for each prediction request.\n",
    "- `predictions`: The predicted confidence, between 0 and 1, per class label.\n",
    "- `deployed_model_id`: The Vertex AI identifier for the deployed `Model` resource which did the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict_request:mbsdk,custom,lcn"
   },
   "outputs": [],
   "source": [
    "instances_list = [INSTANCE]\n",
    "\n",
    "prediction = endpoint.predict(instances_list)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "## Undeploy the model\n",
    "\n",
    "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if \"dataset\" in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if \"model\" in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if \"endpoint\" in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if \"dag\" in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if \"job\" in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if \"batch_predict_job\" in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if \"hpt_job\" in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if \"BUCKET_NAME\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "UJ9 Vertex SDK Custom XGBoost with pre-built training container.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "ligthgbm",
   "language": "python",
   "name": "ligthgbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
