{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Airflow DAG in Cloud Composer from a Vertex Pipeline\n",
    "\n",
    "Apache Airflow is most popular choice for data pipelining in general. However, arguably not a good choice to run Machine learning pipelines due to lack of ML metadata tracking, artifact lineage, tracking ML metrics across metrics etc. [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) solves this problem and automates, monitors, and governs your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata.\n",
    "\n",
    "In this notebook, we will show you how you can trigger a data pipeline i.e. Airflow DAG on Cloud Composer from a ML pipeline running on Vertex Pipelines.\n",
    "\n",
    "![Trigger Airflow DAG on Cloud Composer from Vertex Pipeline](images/trigger-airflow-dag-on-cloud-composer-from-vertex-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are high level steps:\n",
    "\n",
    "1. Create Cloud Composer environment\n",
    "2. Upload Airflow DAG to Composer environment that performs data processing\n",
    "3. Create a Vertex Pipeline that triggers the Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing packages\n",
    "\n",
    "Start with installing KFP SDK and Google Cloud Pipeline components in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing these packages you'll need to restart the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, check that you have correctly installed the packages. The KFP SDK version should be >=1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your project ID and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook you'll reference your Cloud project ID and the bucket you created earlier. Next we'll create variables for each of those.\n",
    "\n",
    "If you don't know your project ID you may be able to get it by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "\n",
    "creds, PROJECT_ID = google.auth.default()\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, set it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PROJECT_ID = {PROJECT_ID}\")\n",
    "print(f\"REGION = {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a variable to store your bucket name and create the bucket if it does not exists already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://\" + \"cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only if the bucket does not exists already\n",
    "!gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Composer Environment\n",
    "\n",
    "Please follow the instructions in the [document](https://cloud.google.com/composer/docs/how-to/managing/creating#) to create a Composer Environment with the configuration you need. For this sample demonstration, we create a bare minimum Composer environment. \n",
    "\n",
    "To trigger an Airflow DAG from Verte Pipeline, we will using Airflow web server REST API. By default, the API authentication feature is disabled in Airflow 1.10.11 and above which would deny all requests made to Airflow web server. To trigger DAG, we enable this feature. To enable the API authentication feature we override `auth_backend` configuration in Composer environment to `airflow.api.auth.backend.default`.\n",
    "\n",
    "**NOTE:** Cloud Composer environment creation may take up to 30 min. Grab your favorite beverage until then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOSER_ENV_NAME = \"test-composer-env\"\n",
    "ZONE = \"us-central1-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta composer environments create $COMPOSER_ENV_NAME \\\n",
    "    --location $REGION \\\n",
    "    --zone $ZONE\\\n",
    "    --machine-type n1-standard-2 \\\n",
    "    --image-version composer-latest-airflow-1.10.15 \\\n",
    "    --airflow-configs=api-auth_backend=airflow.api.auth.backend.default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Composer Environment configuration\n",
    "\n",
    "We will get Composer environment configuration such as webserver URL and client ID to use in the Vertex Pipeline using the script `get_composer_client_id.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is modified version of https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/composer/rest/get_client_id.py\n",
    "\n",
    "shell_output=!python3 get_composer_config.py $PROJECT_ID $REGION $COMPOSER_ENV_NAME\n",
    "COMPOSER_WEB_URI = shell_output[0]\n",
    "COMPOSER_DAG_GCS = shell_output[1]\n",
    "COMPOSER_CLIENT_ID = shell_output[2]\n",
    "\n",
    "print(f\"COMPOSER_WEB_URI = {COMPOSER_WEB_URI}\")\n",
    "print(f\"COMPOSER_DAG_GCS = {COMPOSER_DAG_GCS}\")\n",
    "print(f\"COMPOSER_CLIENT_ID = {COMPOSER_CLIENT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate to Airflow webserver by going to this URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOSER_WEB_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload DAG to Cloud Composer environment\n",
    "\n",
    "We have a sample data processing DAG `data_orchestration_bq_example_dag.py` that reads a CSV file from GCS bucket and writes to BigQuery. We will add this file to the GCS bucket configure for the Composer environment that Airflow watches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOSER_DAG_NAME = \"dag_gcs_to_bq_orch\"\n",
    "COMPOSER_DAG_FILENAME = \"data_orchestration_bq_example_dag.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $COMPOSER_DAG_FILENAME\n",
    "\n",
    "\"\"\"An example Composer workflow integrating GCS and BigQuery.\n",
    "\n",
    "A .csv is read from a GCS bucket to a BigQuery table; a query is made, and the\n",
    "result is written back to a different BigQuery table within a new dataset.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "YESTERDAY = datetime.combine(\n",
    "    datetime.today() - timedelta(days=1), datetime.min.time())\n",
    "BQ_DATASET_NAME = 'bq_demos'\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': YESTERDAY,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Solution: pass a schedule_interval argument to DAG instantiation.\n",
    "with DAG('dag_gcs_to_bq_orch', default_args=default_args,\n",
    "         schedule_interval=None) as dag:\n",
    "  create_bq_dataset_if_not_exist = \"\"\"\n",
    "    bq ls {0}\n",
    "    if [ $? -ne 0 ]; then\n",
    "      bq mk {0}\n",
    "    fi\n",
    "  \"\"\".format(BQ_DATASET_NAME)\n",
    "\n",
    "  # Create destination dataset.\n",
    "  t1 = BashOperator(\n",
    "      task_id='create_destination_dataset',\n",
    "      bash_command=create_bq_dataset_if_not_exist,\n",
    "      dag=dag)\n",
    "\n",
    "  # Create a bigquery table from a .csv file located in a GCS bucket\n",
    "  # (gs://example-datasets/game_data_condensed.csv).\n",
    "  # Store it in our dataset.\n",
    "  t2 = GoogleCloudStorageToBigQueryOperator(\n",
    "      task_id='gcs_to_bq',\n",
    "      bucket='example-datasets',\n",
    "      source_objects=['game_data_condensed.csv'],\n",
    "      destination_project_dataset_table='{0}.composer_game_data_table'\n",
    "      .format(BQ_DATASET_NAME),\n",
    "      schema_fields=[\n",
    "          {'name': 'name', 'type': 'string', 'mode': 'nullable'},\n",
    "          {'name': 'team', 'type': 'string', 'mode': 'nullable'},\n",
    "          {'name': 'total_score', 'type': 'integer', 'mode': 'nullable'},\n",
    "          {'name': 'timestamp', 'type': 'integer', 'mode': 'nullable'},\n",
    "          {'name': 'window_start', 'type': 'string', 'mode': 'nullable'},\n",
    "      ],\n",
    "      write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "  # Run example query (http://shortn/_BdF1UTEYOb) and save result to the\n",
    "  # destination table.\n",
    "  t3 = BigQueryOperator(\n",
    "      task_id='bq_example_query',\n",
    "      bql=f\"\"\"\n",
    "        SELECT\n",
    "          name, team, total_score\n",
    "        FROM\n",
    "          {BQ_DATASET_NAME}.composer_game_data_table\n",
    "        WHERE total_score > 15\n",
    "        LIMIT 100;\n",
    "      \"\"\",\n",
    "      destination_dataset_table='{0}.gcp_example_query_result'\n",
    "      .format(BQ_DATASET_NAME),\n",
    "      write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "  t1 >> t2 >> t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $COMPOSER_DAG_FILENAME $COMPOSER_DAG_GCS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -l $COMPOSER_DAG_GCS/$COMPOSER_DAG_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should the DAG in your Airflow webserver\n",
    "\n",
    "![](images/airflow_webserver_with_dag.png)\n",
    "\n",
    "![](images/airflow_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex Pipelines setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Add the following to import the libraries we'll be using throughout this codelab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import re\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants\n",
    "\n",
    "Before building the pipeline define some constant variables:\n",
    "\n",
    "- `PIPELINE_ROOT` is the Cloud Storage path where the artifacts created by the pipeline will be written. We're using us-central1 as the region here, but if you used a different region when you created your bucket, update the REGION variable in the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, you should see the root directory for your pipeline printed. This is the Cloud Storage location where the artifacts from your pipeline will be written. It will be in the format of `gs://BUCKET_NAME/pipeline_root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function based component to trigger Airflow DAG\n",
    "\n",
    "Using the KFP SDK, we can create components based on Python functions. The component takes Airflow DAG name `dag_name` a string as input and returns response from Airflow web server as an `Artifact` that contains Airflow DAG run information. The component makes a request to Airflow REST API of your Cloud Composer environment. Airflow processes this request and runs a DAG. The DAG outputs information about the change that is logged as artifact (you can output as string as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.3\",\n",
    "    output_component_file=\"composer-trigger-dag-component.yaml\",\n",
    "    packages_to_install=[\"requests\"],\n",
    ")\n",
    "def trigger_airflow_dag(\n",
    "    dag_name: str,\n",
    "    composer_client_id: str,\n",
    "    composer_webserver_id: str,\n",
    "    response: Output[Artifact]\n",
    "):\n",
    "    # [START composer_trigger]\n",
    "\n",
    "    from google.auth.transport.requests import Request\n",
    "    from google.oauth2 import id_token\n",
    "    import requests\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    IAM_SCOPE = 'https://www.googleapis.com/auth/iam'\n",
    "    OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token'\n",
    "    \n",
    "    data = '{\"replace_microseconds\":\"false\"}'\n",
    "    context = None\n",
    "\n",
    "    \"\"\"Makes a POST request to the Composer DAG Trigger API\n",
    "\n",
    "    When called via Google Cloud Functions (GCF),\n",
    "    data and context are Background function parameters.\n",
    "\n",
    "    For more info, refer to\n",
    "    https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python\n",
    "\n",
    "    To call this function from a Python script, omit the ``context`` argument\n",
    "    and pass in a non-null value for the ``data`` argument.\n",
    "    \"\"\"\n",
    "\n",
    "    # Form webserver URL to make REST API calls\n",
    "    webserver_url = f'{composer_webserver_id}/api/experimental/dags/{dag_name}/dag_runs'\n",
    "    # print(webserver_url)\n",
    "\n",
    "    # This code is copied from\n",
    "    # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py\n",
    "    # START COPIED IAP CODE\n",
    "    def make_iap_request(url, client_id, method='GET', **kwargs):\n",
    "        \"\"\"Makes a request to an application protected by Identity-Aware Proxy.\n",
    "        Args:\n",
    "          url: The Identity-Aware Proxy-protected URL to fetch.\n",
    "          client_id: The client ID used by Identity-Aware Proxy.\n",
    "          method: The request method to use\n",
    "                  ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE')\n",
    "          **kwargs: Any of the parameters defined for the request function:\n",
    "                    https://github.com/requests/requests/blob/master/requests/api.py\n",
    "                    If no timeout is provided, it is set to 90 by default.\n",
    "        Returns:\n",
    "          The page body, or raises an exception if the page couldn't be retrieved.\n",
    "        \"\"\"\n",
    "        # Set the default timeout, if missing\n",
    "        if 'timeout' not in kwargs:\n",
    "            kwargs['timeout'] = 90\n",
    "\n",
    "        # Obtain an OpenID Connect (OIDC) token from metadata server or using service\n",
    "        # account.\n",
    "        google_open_id_connect_token = id_token.fetch_id_token(Request(), client_id)\n",
    "\n",
    "        # Fetch the Identity-Aware Proxy-protected URL, including an\n",
    "        # Authorization header containing \"Bearer \" followed by a\n",
    "        # Google-issued OpenID Connect token for the service account.\n",
    "        resp = requests.request(\n",
    "            method, url,\n",
    "            headers={'Authorization': 'Bearer {}'.format(\n",
    "                google_open_id_connect_token)}, **kwargs)\n",
    "        if resp.status_code == 403:\n",
    "            raise Exception('Service account does not have permission to '\n",
    "                            'access the IAP-protected application.')\n",
    "        elif resp.status_code != 200:\n",
    "            raise Exception(\n",
    "                'Bad response from application: {!r} / {!r} / {!r}'.format(\n",
    "                    resp.status_code, resp.headers, resp.text))\n",
    "        else:\n",
    "            print(f\"response = {resp.text}\")\n",
    "            file_path = os.path.join(response.path)\n",
    "            os.makedirs(file_path)\n",
    "            with open(os.path.join(file_path, \"airflow_response.json\"), 'w') as f:\n",
    "                json.dump(resp.text, f)\n",
    "\n",
    "    # END COPIED IAP CODE\n",
    "\n",
    "    \n",
    "    # Make a POST request to IAP which then Triggers the DAG\n",
    "    make_iap_request(\n",
    "        webserver_url, composer_client_id, method='POST', json={\"conf\": data, \"replace_microseconds\": 'false'})\n",
    "    \n",
    "    # [END composer_trigger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the component structure\n",
    "- The **`@component`** decorator compiles this function to a component when the pipeline is run. You'll use this anytime you write a custom component.\n",
    "- The **`base_image parameter`** specifies the container image this component will use.\n",
    "- The **`output_component_file`** parameter is optional, and specifies the yaml file to write the compiled component to.\n",
    "- The **`packages_to_install`** parameter installs required python packages in the container to run the component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Triggering Airflow DAG from Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running comment out @component annotation in the cell above\n",
    "trigger_airflow_dag(\n",
    "    dag_name=COMPOSER_DAG_NAME,\n",
    "    composer_client_id=COMPOSER_CLIENT_ID,\n",
    "    composer_webserver_id=COMPOSER_WEB_URI,\n",
    "    response=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOSER_WEB_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the components to a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"pipeline-trigger-airflow-dag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"Trigger Airflow DAG from Vertex Pipelines\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# You can change the `text` and `emoji_str` parameters here to update the pipeline output\n",
    "def pipeline():\n",
    "    data_processing_task_dag_name = COMPOSER_DAG_NAME\n",
    "    data_processing_task = trigger_airflow_dag(\n",
    "        dag_name=data_processing_task_dag_name,\n",
    "        composer_client_id=COMPOSER_CLIENT_ID,\n",
    "        composer_webserver_id=COMPOSER_WEB_URI\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your pipeline defined, you're ready to compile it. The following will generate a JSON file that you'll use to run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=f\"{PIPELINE_NAME}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate an API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=f\"{PIPELINE_NAME}.json\",\n",
    "    # pipeline_root=PIPELINE_ROOT  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Vertex Pipeline status\n",
    "\n",
    "From Cloud Console, you can monitor the pipeline run status and view the output artifact\n",
    "\n",
    "![](images/pipeline_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also API client to get pipeline status and artifact information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_id(job_name):\n",
    "    \"\"\"get job id from pipeline job name\"\"\"\n",
    "    p = re.compile('projects/(?P<project_id>.*)/locations/(?P<region>.*)/pipelineJobs/(?P<job_id>.*)')\n",
    "    result = p.search(job_name)\n",
    "    return result.group('job_id') if result else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_status = api_client.get_job(get_job_id(response['name']))\n",
    "print(f\"JOB STATUS: {job_status['state']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Airflow DAG run instance from the output artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_response_uri = [task['outputs']['response']['artifacts'][0]['uri'] for task in job_status['jobDetail']['taskDetails'] if task['taskName']=='trigger-airflow-dag'][0]\n",
    "airflow_response_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $airflow_response_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $airflow_response_uri/airflow_response.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Airflow DAG run\n",
    "\n",
    "Go to Airflow webserver and monitor the status of data processing DAG. Airflow webserver URL is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPOSER_WEB_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/airflow_dag_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete Cloud Storage bucket\n",
    "- Delete Cloud Composer environment"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "vertex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
