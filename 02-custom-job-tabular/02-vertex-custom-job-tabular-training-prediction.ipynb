{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88dd700",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb2d08",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977785fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user google-cloud-aiplatform\n",
    "pip install --user kfp\n",
    "pip install --user google-cloud-pipeline-components\n",
    "pip install --user google-cloud-bigquery-datatransfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b0041",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "Once you've installed the required packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abb230",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0a612338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import google.auth\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2f4fe",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0eaa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT = rt-vertex-sandbox\n",
      "STAGING_BUCKET = gs://rtvw-rt-vertex-sandbox-bucket\n",
      "VERTEX_SA = training-sa@rt-vertex-sandbox.iam.gserviceaccount.com\n",
      "TENSORBOARD = projects/437422844431/locations/us-central1/tensorboards/6509988445736140800\n"
     ]
    }
   ],
   "source": [
    "creds, PROJECT = google.auth.default()\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'rtvw'            # <--- CHANGE THIS TO VARIABLE YOU SET UP DURING PREREQUISITES\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-{PROJECT}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "print(f\"PROJECT = {PROJECT}\")\n",
    "print(f\"STAGING_BUCKET = {STAGING_BUCKET}\")\n",
    "print(f\"VERTEX_SA = {VERTEX_SA}\")\n",
    "\n",
    "# get tensorboard instance\n",
    "shell_output = !gcloud beta ai tensorboards list --region $REGION --filter=displayName:$PREFIX-$REGION-tensorboard --format='value(name)' --quiet 2>/dev/null\n",
    "TENSORBOARD = shell_output[-1]\n",
    "print(f\"TENSORBOARD = {TENSORBOARD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8cfde",
   "metadata": {},
   "source": [
    "## Preparing training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e6b27",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d11e3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 633.77query/s] \n",
      "Downloading: 100%|██████████| 3/3 [00:02<00:00,  1.37rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea35986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unique_key</th>\n",
       "      <td>afdb4903f8cbe066868d90c06b01449f58052462</td>\n",
       "      <td>a9488bef4cecbda3c212a6ebcd261ba26749df36</td>\n",
       "      <td>dcbbad2d636c999ef7fe5bbda536fb5ee3b84f8a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxi_id</th>\n",
       "      <td>4399985da5ca143d850858bc197bf4a744366cfc19bb86...</td>\n",
       "      <td>68d140be0fa68d84f89f8ef45edca6eafd824b0bc30660...</td>\n",
       "      <td>dfdb89329a6bd26bb76e1d13bcac71b813d6bd3c9f3439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <td>2015-07-03 01:15:00+00:00</td>\n",
       "      <td>2015-10-04 01:30:00+00:00</td>\n",
       "      <td>2015-06-18 21:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <td>2015-07-03 01:15:00+00:00</td>\n",
       "      <td>2015-10-04 01:30:00+00:00</td>\n",
       "      <td>2015-06-18 21:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_seconds</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_miles</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_community_area</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare</th>\n",
       "      <td>8.45</td>\n",
       "      <td>40.0</td>\n",
       "      <td>69.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tips</th>\n",
       "      <td>2.11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extras</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_total</th>\n",
       "      <td>10.56</td>\n",
       "      <td>45.0</td>\n",
       "      <td>86.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "      <td>Suburban Dispatch LLC</td>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_location</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_location</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        0  \\\n",
       "unique_key                       afdb4903f8cbe066868d90c06b01449f58052462   \n",
       "taxi_id                 4399985da5ca143d850858bc197bf4a744366cfc19bb86...   \n",
       "trip_start_timestamp                            2015-07-03 01:15:00+00:00   \n",
       "trip_end_timestamp                              2015-07-03 01:15:00+00:00   \n",
       "trip_seconds                                                            0   \n",
       "trip_miles                                                            0.0   \n",
       "pickup_census_tract                                                   NaN   \n",
       "dropoff_census_tract                                                  NaN   \n",
       "pickup_community_area                                                 NaN   \n",
       "dropoff_community_area                                                NaN   \n",
       "fare                                                                 8.45   \n",
       "tips                                                                 2.11   \n",
       "tolls                                                                 0.0   \n",
       "extras                                                                0.0   \n",
       "trip_total                                                          10.56   \n",
       "payment_type                                                  Credit Card   \n",
       "company                          Chicago Elite Cab Corp. (Chicago Carriag   \n",
       "pickup_latitude                                                       NaN   \n",
       "pickup_longitude                                                      NaN   \n",
       "pickup_location                                                      None   \n",
       "dropoff_latitude                                                      NaN   \n",
       "dropoff_longitude                                                     NaN   \n",
       "dropoff_location                                                     None   \n",
       "\n",
       "                                                                        1  \\\n",
       "unique_key                       a9488bef4cecbda3c212a6ebcd261ba26749df36   \n",
       "taxi_id                 68d140be0fa68d84f89f8ef45edca6eafd824b0bc30660...   \n",
       "trip_start_timestamp                            2015-10-04 01:30:00+00:00   \n",
       "trip_end_timestamp                              2015-10-04 01:30:00+00:00   \n",
       "trip_seconds                                                            0   \n",
       "trip_miles                                                            0.0   \n",
       "pickup_census_tract                                                   NaN   \n",
       "dropoff_census_tract                                                  NaN   \n",
       "pickup_community_area                                                 NaN   \n",
       "dropoff_community_area                                                NaN   \n",
       "fare                                                                 40.0   \n",
       "tips                                                                  5.0   \n",
       "tolls                                                                 0.0   \n",
       "extras                                                                0.0   \n",
       "trip_total                                                           45.0   \n",
       "payment_type                                                  Credit Card   \n",
       "company                                             Suburban Dispatch LLC   \n",
       "pickup_latitude                                                       NaN   \n",
       "pickup_longitude                                                      NaN   \n",
       "pickup_location                                                      None   \n",
       "dropoff_latitude                                                      NaN   \n",
       "dropoff_longitude                                                     NaN   \n",
       "dropoff_location                                                     None   \n",
       "\n",
       "                                                                        2  \n",
       "unique_key                       dcbbad2d636c999ef7fe5bbda536fb5ee3b84f8a  \n",
       "taxi_id                 dfdb89329a6bd26bb76e1d13bcac71b813d6bd3c9f3439...  \n",
       "trip_start_timestamp                            2015-06-18 21:00:00+00:00  \n",
       "trip_end_timestamp                              2015-06-18 21:00:00+00:00  \n",
       "trip_seconds                                                            0  \n",
       "trip_miles                                                            0.0  \n",
       "pickup_census_tract                                                   NaN  \n",
       "dropoff_census_tract                                                  NaN  \n",
       "pickup_community_area                                                 NaN  \n",
       "dropoff_community_area                                                NaN  \n",
       "fare                                                                69.45  \n",
       "tips                                                                17.36  \n",
       "tolls                                                                 0.0  \n",
       "extras                                                                0.0  \n",
       "trip_total                                                          86.81  \n",
       "payment_type                                                  Credit Card  \n",
       "company                          Chicago Elite Cab Corp. (Chicago Carriag  \n",
       "pickup_latitude                                                       NaN  \n",
       "pickup_longitude                                                      NaN  \n",
       "pickup_location                                                      None  \n",
       "dropoff_latitude                                                      NaN  \n",
       "dropoff_longitude                                                     NaN  \n",
       "dropoff_location                                                     None  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b82e8686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 658.65query/s] \n",
      "Downloading: 100%|██████████| 7/7 [00:01<00:00,  6.03rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2020 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f2c4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_dayofweek</th>\n",
       "      <th>trip_dayname</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>334984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monday</td>\n",
       "      <td>548213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>599434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>646244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>686346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Friday</td>\n",
       "      <td>667116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>406695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_dayofweek trip_dayname  trip_count\n",
       "0              1       Sunday      334984\n",
       "1              2       Monday      548213\n",
       "2              3      Tuesday      599434\n",
       "3              4    Wednesday      646244\n",
       "4              5     Thursday      686346\n",
       "5              6       Friday      667116\n",
       "6              7     Saturday      406695"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c807704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='trip_dayname'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a844fb",
   "metadata": {},
   "source": [
    "### Create  data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1dcd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "BQ_LOCATION = 'US'\n",
    "SAMPLE_SIZE = 500000\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a299eb8",
   "metadata": {},
   "source": [
    "#### Create a BQ dataset to host the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2b3a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rt-vertex-sandbox.rtvw_dataset already exists\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(location=BQ_LOCATION)\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fd2dd",
   "metadata": {},
   "source": [
    "#### Create training, validation, and test splits tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3f1b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script_template = '''\n",
    "CREATE TEMP TABLE features \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'TEST'\n",
    "          WHEN 8 THEN 'VALIDATE'\n",
    "          ELSE 'TRAIN' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TRAIN';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='VALIDATE';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TEST';\n",
    "\n",
    "DROP TABLE features;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0af1f7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f2007cffa90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_script = sql_script_template.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TRAIN_SPLIT', BQ_TRAIN_SPLIT_NAME).replace(\n",
    "    '@VALIDATE_SPLIT', BQ_VALID_SPLIT_NAME).replace(\n",
    "    '@TEST_SPLIT', BQ_TEST_SPLIT_NAME).replace(\n",
    "    '@YEAR', str(YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE))\n",
    "\n",
    "job = client.query(sql_script)\n",
    "job.result()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ac211",
   "metadata": {},
   "source": [
    "#### Review splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66381d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM `{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}`\n",
    "'''\n",
    "\n",
    "data = client.query(sql_script).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd315f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_month</th>\n",
       "      <th>trip_day</th>\n",
       "      <th>trip_day_of_week</th>\n",
       "      <th>trip_hour</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>pickup_grid</th>\n",
       "      <th>dropoff_grid</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>tip_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>165</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.7 42)</td>\n",
       "      <td>POINT(-87.7 42)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>360</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>720</td>\n",
       "      <td>0.10</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>0.17</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>292</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_month  trip_day  trip_day_of_week  trip_hour  trip_seconds  \\\n",
       "0           2         1                 7          8           165   \n",
       "1           2         1                 7         16           360   \n",
       "2           2         1                 7         19           720   \n",
       "3           2         1                 7          2           126   \n",
       "4           2         1                 7         18           292   \n",
       "\n",
       "   trip_miles payment_type        pickup_grid       dropoff_grid  euclidean  \\\n",
       "0        0.62         Cash    POINT(-87.7 42)    POINT(-87.7 42)        0.0   \n",
       "1        1.00         Cash  POINT(-87.6 41.9)  POINT(-87.6 41.9)        0.0   \n",
       "2        0.10  Credit Card  POINT(-87.7 41.9)  POINT(-87.7 41.9)        0.0   \n",
       "3        0.17  Credit Card  POINT(-87.7 41.9)  POINT(-87.7 41.9)        0.0   \n",
       "4        0.29         Cash  POINT(-87.7 41.9)  POINT(-87.7 41.9)        0.0   \n",
       "\n",
       "   tip_bin  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b56dc4",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d214d57",
   "metadata": {},
   "source": [
    "![Training pipeline](../images/custom-training-on-vertex-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a2672",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b63e1",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "237f5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b0b09bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    history = model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad6219",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK and Set an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784204ee",
   "metadata": {},
   "source": [
    "Define experiment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ca8f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"chicago-taxi-tips-classifier\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcd023",
   "metadata": {},
   "source": [
    "If EXPERIMENT_NAME is not set, set a default one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a50b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "661f000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_NAME == \"\" or EXPERIMENT_NAME is None:\n",
    "    EXPERIMENT_NAME = \"my-experiment-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62564589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Resource chicago-taxi-tips-classifier not found.\n",
      "INFO:root:Creating Resource chicago-taxi-tips-classifier\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069abb1a",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f72ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/jobs/chicago-taxi-clsfr-custom-20210620155233/aiplatform-2021-06-20-16:04:44.701-aiplatform_custom_trainer_script-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-{TIMESTAMP}\"\n",
    "base_output_dir = f'{STAGING_BUCKET}/jobs/{job_name}'\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    f'--epochs={epochs}', \n",
    "    f'--per_replica_batch_size={batch_size}',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "\n",
    "vertex_ai.start_run(\"custom-training-run-1\")  # Change this to your desired run name\n",
    "parameters = {\"epochs\": 2, \n",
    "              \"batch_size\": 128,\n",
    "              \"training_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "              \"validation_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}'\n",
    "             }\n",
    "vertex_ai.log_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a99a3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/437422844431/locations/us-central1/customJobs/8371527602258575360\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/437422844431/locations/us-central1/customJobs/8371527602258575360')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8371527602258575360?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+437422844431+locations+us-central1+tensorboards+6509988445736140800+experiments+8371527602258575360\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/8371527602258575360 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/8371527602258575360 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/8371527602258575360 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/8371527602258575360 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eba35929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: chicago-taxi-clsfr-custom-20210620155233\n",
      "Job Resource Name: projects/437422844431/locations/us-central1/customJobs/8371527602258575360\n",
      "\n",
      "Check training progress at https://console.cloud.google.com/ai/platform/locations/us-central1/training/8371527602258575360?project=437422844431\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {job.display_name}\")\n",
    "print(f\"Job Resource Name: {job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989df050",
   "metadata": {},
   "source": [
    "Check model artifacts in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "486ffea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rtvw-rt-vertex-sandbox-bucket/jobs/chicago-taxi-clsfr-custom-20210620155233/model/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/jobs/chicago-taxi-clsfr-custom-20210620155233/model/saved_model.pb\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/jobs/chicago-taxi-clsfr-custom-20210620155233/model/assets/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/jobs/chicago-taxi-clsfr-custom-20210620155233/model/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $base_output_dir/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c756d",
   "metadata": {},
   "source": [
    "Model artifacts to be deployed to Vertex Model Resource later in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6e5e1",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0b04a",
   "metadata": {},
   "source": [
    "#### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7db15b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_chicago_taxi_clsfr_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a5dd6",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b506acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 9.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://rt-vertex-sandbox_cloudbuild/source/1624231954.886997-44d1f8689a6442f0b8342b492d52f129.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rt-vertex-sandbox/locations/global/builds/c0405476-7334-4bc5-a091-0b9210469c9a].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c0405476-7334-4bc5-a091-0b9210469c9a?project=437422844431].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c0405476-7334-4bc5-a091-0b9210469c9a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rt-vertex-sandbox_cloudbuild/source/1624231954.886997-44d1f8689a6442f0b8342b492d52f129.tgz#1624231955632060\n",
      "Copying gs://rt-vertex-sandbox_cloudbuild/source/1624231954.886997-44d1f8689a6442f0b8342b492d52f129.tgz#1624231955632060...\n",
      "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
      "Operation completed over 1 objects/3.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "424e7c9d5d89: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "803ecb627365: Waiting\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "c631c38965fd: Waiting\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Verifying Checksum\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Verifying Checksum\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Verifying Checksum\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "16a6777d4439: Verifying Checksum\n",
      "16a6777d4439: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "4466b5c2ac37: Verifying Checksum\n",
      "4466b5c2ac37: Download complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "803ecb627365: Download complete\n",
      "9b397537aef0: Pull complete\n",
      "2bd5028f4b85: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b2ed56b85d3a: Pull complete\n",
      "8bfb788e9874: Pull complete\n",
      "0618fb353339: Pull complete\n",
      "42045a665612: Pull complete\n",
      "031d8d7b75f7: Pull complete\n",
      "5780cc9addac: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "803ecb627365: Pull complete\n",
      "4466b5c2ac37: Pull complete\n",
      "16a6777d4439: Pull complete\n",
      "7a84944cecd7: Pull complete\n",
      "Digest: sha256:56489bdb00b3f5c342d8cbcdeea68fd63b982858199c4ed7edac15614954b110\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-4:latest\n",
      " ---> fbf8f454dc05\n",
      "Step 2/5 : WORKDIR /trainer\n",
      " ---> Running in a66508ffaf37\n",
      "Removing intermediate container a66508ffaf37\n",
      " ---> 1d92ed4fa6cb\n",
      "Step 3/5 : RUN pip install cloudml-hypertune\n",
      " ---> Running in cc730604eb70\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=45bd0e55214f69b5b58d5f21471e5d1b89d8048dc0b1de3fbfb8198b36dd5f37\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container cc730604eb70\n",
      " ---> af3607f985a4\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 33f6c0070e60\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 5dfe943c63bd\n",
      "Removing intermediate container 5dfe943c63bd\n",
      " ---> 9878c14c22b7\n",
      "Successfully built 9878c14c22b7\n",
      "Successfully tagged gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_trainer\n",
      "The push refers to repository [gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_trainer]\n",
      "e27b41e46e67: Preparing\n",
      "cdc4143b8dd5: Preparing\n",
      "263406235c4c: Preparing\n",
      "d39fae2e5e4e: Preparing\n",
      "33302bd81efa: Preparing\n",
      "0c5f899e9bc7: Preparing\n",
      "5e593cdd4a12: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "42b027d1e826: Preparing\n",
      "636a7c2e7d03: Preparing\n",
      "1ba1158adf89: Preparing\n",
      "96e46d1341e8: Preparing\n",
      "954f6dc3f7f5: Preparing\n",
      "8760a171b659: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0710233fd2d: Preparing\n",
      "05449afa4be9: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "0c5f899e9bc7: Waiting\n",
      "5e593cdd4a12: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "42b027d1e826: Waiting\n",
      "636a7c2e7d03: Waiting\n",
      "1ba1158adf89: Waiting\n",
      "96e46d1341e8: Waiting\n",
      "954f6dc3f7f5: Waiting\n",
      "8760a171b659: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a0710233fd2d: Waiting\n",
      "05449afa4be9: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "33302bd81efa: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "d39fae2e5e4e: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "0c5f899e9bc7: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "5e593cdd4a12: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "e27b41e46e67: Pushed\n",
      "263406235c4c: Pushed\n",
      "cdc4143b8dd5: Pushed\n",
      "06a5bf49b163: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "b34dae69fc5d: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "0ffb7465dde9: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "e2563d1ada9a: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "42b027d1e826: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "636a7c2e7d03: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "1ba1158adf89: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "96e46d1341e8: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "5f70bf18a086: Layer already exists\n",
      "8760a171b659: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "954f6dc3f7f5: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "a0710233fd2d: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "05449afa4be9: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5b9e34b5cf74: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "5153e1acaabc: Layer already exists\n",
      "latest: digest: sha256:cb1ec2ea7e8ac0974b51f264565bba57c69c0380f500fdf41577618caeb9af14 size: 5335\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                              STATUS\n",
      "c0405476-7334-4bc5-a091-0b9210469c9a  2021-06-20T23:32:35+00:00  2M19S     gs://rt-vertex-sandbox_cloudbuild/source/1624231954.886997-44d1f8689a6442f0b8342b492d52f129.tgz  gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_trainer (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e88faf",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c886b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                f'--epochs={epochs}', \n",
    "                f'--per_replica_batch_size={batch_size}',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646e5e2",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cd197b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Resource chicago-taxi-tips-classifier-custom-training-run-2 not found.\n",
      "INFO:root:Creating Resource chicago-taxi-tips-classifier-custom-training-run-2\n",
      "INFO:root:Resource chicago-taxi-tips-classifier-custom-training-run-2-metrics not found.\n",
      "INFO:root:Creating Resource chicago-taxi-tips-classifier-custom-training-run-2-metrics\n"
     ]
    }
   ],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")\n",
    "\n",
    "vertex_ai.start_run(\"custom-training-run-2\")  # Change this to your desired run name\n",
    "parameters = {\"epochs\": epochs, \n",
    "              \"batch_size\": batch_size,\n",
    "              \"training_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "              \"validation_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}'\n",
    "             }\n",
    "vertex_ai.log_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30edb33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/437422844431/locations/us-central1/customJobs/4977502333081485312\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/437422844431/locations/us-central1/customJobs/4977502333081485312')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4977502333081485312?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+437422844431+locations+us-central1+tensorboards+6509988445736140800+experiments+4977502333081485312\n"
     ]
    }
   ],
   "source": [
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "379a3648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: chicago-taxi-clsfr-custom-20210620_235910\n",
      "Job Resource Name: projects/437422844431/locations/us-central1/customJobs/4977502333081485312\n",
      "\n",
      "Check training progress at https://console.cloud.google.com/ai/platform/locations/us-central1/training/4977502333081485312?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {job.display_name}\")\n",
    "print(f\"Job Resource Name: {job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf23548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/4977502333081485312 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {STAGING_BUCKET}/{job_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823b1b",
   "metadata": {},
   "source": [
    "### Configure and submit a Hyperparameter Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d23e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# The spec of the worker pools including machine type and Docker image\n",
    "# Be sure to replace IMAGE_URI with the path to your Docker image in GCR\n",
    "worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-4\",\n",
    "      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "      #  \"accelerator_count\": 1,\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": TRAIN_IMAGE,\n",
    "        \"args\": [\n",
    "            f'--epochs={epochs}', \n",
    "            f'--per_replica_batch_size={batch_size}',\n",
    "            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "        ],\n",
    "    },\n",
    "}]\n",
    "\n",
    "# Dicionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec={'val_accuracy':'maximize'}\n",
    "\n",
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"units\": hpt.DiscreteParameterSpec(values=[32, 64], scale=None),\n",
    "    \"dropout_ratio\": hpt.DoubleParameterSpec(min=0.4, max=0.6, scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668353d",
   "metadata": {},
   "source": [
    "#### Configure custom job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d283a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2268ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-hp-{TIMESTAMP}\"\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a29bf0",
   "metadata": {},
   "source": [
    "#### Create and run Hyperparameter Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7120cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_job_name = f\"chicago-taxi-clsfr-hptuning-{TIMESTAMP}\"\n",
    "\n",
    "hp_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=hp_job_name,\n",
    "    custom_job=job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=3,\n",
    "    parallel_trial_count=3,\n",
    "    max_failed_trial_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e750e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/840946075341684736?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+437422844431+locations+us-central1+tensorboards+6509988445736140800+experiments+840946075341684736\n"
     ]
    }
   ],
   "source": [
    "hp_job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59274e9",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61d5efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: chicago-taxi-clsfr-hptuning-20210621_002500\n",
      "Job Resource Name: projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736\n",
      "\n",
      "Check training progress at https://console.cloud.google.com/ai/platform/locations/us-central1/training/840946075341684736?project=437422844431\n",
      "Job state 2\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob run completed. Resource name: projects/437422844431/locations/us-central1/hyperparameterTuningJobs/840946075341684736\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {hp_job.display_name}\")\n",
    "print(f\"Job Resource Name: {hp_job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {hp_job._dashboard_uri()}\")\n",
    "\n",
    "print(f\"Job state {hp_job.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb8131",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44675054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study trials have completed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if hp_job.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", hp_job.state)\n",
    "        if (hp_job.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           hp_job.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5afac6",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6246ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>parameters</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>{'dropout_ratio': 0.5, 'units': 64.0}</td>\n",
       "      <td>2021-06-21T00:26:12.724231184Z</td>\n",
       "      <td>2021-06-21T00:50:16Z</td>\n",
       "      <td>0.875409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>{'dropout_ratio': 0.5438352239209705, 'units': 32.0}</td>\n",
       "      <td>2021-06-21T00:26:12.724453137Z</td>\n",
       "      <td>2021-06-21T00:46:10Z</td>\n",
       "      <td>0.875096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>{'dropout_ratio': 0.5912787023143605, 'units': 64.0}</td>\n",
       "      <td>2021-06-21T00:26:12.724523361Z</td>\n",
       "      <td>2021-06-21T00:49:57Z</td>\n",
       "      <td>0.874701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id      state                                            parameters  \\\n",
       "0  1  SUCCEEDED                 {'dropout_ratio': 0.5, 'units': 64.0}   \n",
       "1  2  SUCCEEDED  {'dropout_ratio': 0.5438352239209705, 'units': 32.0}   \n",
       "2  3  SUCCEEDED  {'dropout_ratio': 0.5912787023143605, 'units': 64.0}   \n",
       "\n",
       "                        startTime               endTime   metrics  \n",
       "0  2021-06-21T00:26:12.724231184Z  2021-06-21T00:50:16Z  0.875409  \n",
       "1  2021-06-21T00:26:12.724453137Z  2021-06-21T00:46:10Z  0.875096  \n",
       "2  2021-06-21T00:26:12.724523361Z  2021-06-21T00:49:57Z  0.874701  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_hp_job_results = pd.DataFrame([MessageToDict(trial.__class__.pb(trial)) for trial in hp_job.trials])\n",
    "df_hp_job_results[\"parameters\"] = df_hp_job_results.parameters.apply(lambda x: {item[\"parameterId\"]:item[\"value\"] for item in x})\n",
    "df_hp_job_results[\"metrics\"] = df_hp_job_results.finalMeasurement.apply(lambda x: x[\"metrics\"][0][\"value\"])\n",
    "df_hp_job_results.drop(\"finalMeasurement\", axis=1, inplace=True)\n",
    "df_hp_job_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "25407cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>parameters</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>{'dropout_ratio': 0.5, 'units': 64.0}</td>\n",
       "      <td>2021-06-21T00:26:12.724231184Z</td>\n",
       "      <td>2021-06-21T00:50:16Z</td>\n",
       "      <td>0.875409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id      state                             parameters  \\\n",
       "0  1  SUCCEEDED  {'dropout_ratio': 0.5, 'units': 64.0}   \n",
       "\n",
       "                        startTime               endTime   metrics  \n",
       "0  2021-06-21T00:26:12.724231184Z  2021-06-21T00:50:16Z  0.875409  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = df_hp_job_results.iloc[[df_hp_job_results[\"metrics\"].idxmax()]]\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3be790",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "066fe06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/2/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/3/\n"
     ]
    }
   ],
   "source": [
    "model_dir = f'{STAGING_BUCKET}/{job_name}'\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3b24f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/checkpoints/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/logs/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id.values[0])\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e3d3d",
   "metadata": {},
   "source": [
    "## Deploying a model to Vertex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2f4c2",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b06c73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dropoff_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_dropoff_grid:0\n",
      "  inputs['euclidean'] tensor_info:\n",
      "      dtype: DT_DOUBLE\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_euclidean:0\n",
      "  inputs['payment_type'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_payment_type:0\n",
      "  inputs['pickup_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_pickup_grid:0\n",
      "  inputs['trip_day'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day:0\n",
      "  inputs['trip_day_of_week'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day_of_week:0\n",
      "  inputs['trip_hour'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_hour:0\n",
      "  inputs['trip_miles'] tensor_info:\n",
      "      dtype: DT_DOUBLE\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_miles:0\n",
      "  inputs['trip_month'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_month:0\n",
      "  inputs['trip_seconds'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_seconds:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = f'{best_model_dir}/model'\n",
    "\n",
    "!saved_model_cli show --dir {saved_model_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf925a4",
   "metadata": {},
   "source": [
    "### Upload the model using pre-built serving container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5600fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/437422844431/locations/us-central1/models/174804756630339584/operations/6314904571521007616\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/437422844431/locations/us-central1/models/174804756630339584\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/437422844431/locations/us-central1/models/174804756630339584')\n"
     ]
    }
   ],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier'\n",
    "description = 'Chicago Taxi Tip TensorFlow classifier'\n",
    "serving_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest'\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    artifact_uri=saved_model_path,\n",
    "    serving_container_image_uri=serving_image_uri\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589b8b4",
   "metadata": {},
   "source": [
    "### Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0d1ee904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/437422844431/locations/us-central1/endpoints/3346508774671122432/operations/3810903178703011840\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/437422844431/locations/us-central1/endpoints/3346508774671122432\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/437422844431/locations/us-central1/endpoints/3346508774671122432')\n"
     ]
    }
   ],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Endpoint'\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41e256",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/437422844431/locations/us-central1/endpoints/3346508774671122432\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/437422844431/locations/us-central1/endpoints/3346508774671122432/operations/27879491711795200\n"
     ]
    }
   ],
   "source": [
    "deployed_model_display_name = f'{PREFIX}-taxi-v1'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00002f0e",
   "metadata": {},
   "source": [
    "## Invoking the deployed model using Vertex SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36d0c5",
   "metadata": {},
   "source": [
    "### Get the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "86bad969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Endpoint object at 0x7f1f8c706a90> \n",
      "resource name: projects/437422844431/locations/us-central1/endpoints/3346508774671122432\n"
     ]
    }
   ],
   "source": [
    "filter = f'display_name=\"{PREFIX} Chicago Taxi Tip Classifier Endpoint\"'\n",
    "\n",
    "for endpoint_info in vertex_ai.Endpoint.list(filter=filter):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2036452",
   "metadata": {},
   "source": [
    "### Call the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "62565c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of tip > 20%: [[0.7524805]]\n"
     ]
    }
   ],
   "source": [
    "test_instances = [  \n",
    "    \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf9f3b",
   "metadata": {},
   "source": [
    "## Upload the model using custom serving container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d80ac1",
   "metadata": {},
   "source": [
    "You might want to use a custom serving container with Vertex Predictions for any of the following reasons:\n",
    "\n",
    "- To serve predictions from an ML model trained using a framework other than TensorFlow, scikit-learn, or XGBoost\n",
    "- To preprocess prediction requests or postprocess the predictions generated by your model\n",
    "- To run a prediction server written in a programming language of your choice\n",
    "- To install dependencies that you want to use to customize prediction\n",
    "\n",
    "When you use a custom container, Vertex AI runs a Docker container of your choice on each prediction node. The [container image requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) must meet to be compatible with Vertex AI.\n",
    "\n",
    "Following diagram shows the steps to deploy a model on vertex AI with custom container.\n",
    "\n",
    "![serving with custom container on Vertex AI](../images/serving-with-custom-containers-on-vertex-predictions.png)\n",
    "\n",
    "Let's deploy the previously trained Tensorflow/Keras model on Vertex AI with Tensorflow Serving custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f7d7a",
   "metadata": {},
   "source": [
    "### Create Dockerfile with serving container image\n",
    "\n",
    "Dockerfile image consist of following steps\n",
    "\n",
    "1. Create image from base Tensorflow serving image `tensorflow/serving:2.4`\n",
    "2. Copy model artifacts (Tensorflow savedModel) generated from the training job (available in GCS bucket) in the image\n",
    "3. Run Tensorflow Server process in the backend listening to prediction requests on port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "597d9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'predictor'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "dockerfile_path = os.path.join(SCRIPT_FOLDER, 'Dockerfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebc4e2",
   "metadata": {},
   "source": [
    "#### Download Model Artifacts from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0bc897c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model artifacts from the training job located at gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved model artifacts from the training job located at {saved_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b134be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/saved_model.pb\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/assets/\n",
      "gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "278ff0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = 'model'\n",
    "if tf.io.gfile.exists(MODEL_FOLDER):\n",
    "    tf.io.gfile.rmtree(MODEL_FOLDER)\n",
    "tf.io.gfile.mkdir(MODEL_FOLDER)\n",
    "dockerfile_path = os.path.join(MODEL_FOLDER, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "98c66b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/saved_model.pb...\n",
      "/ [1 files][589.9 KiB/589.9 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/variables/variables.index...\n",
      "/ [3 files][692.8 KiB/692.8 KiB]                                                \n",
      "Operation completed over 3 objects/692.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/* $SCRIPT_FOLDER/$MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a3801",
   "metadata": {},
   "source": [
    "#### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4881e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dockerfile_path}\n",
    "\n",
    "FROM tensorflow/serving\n",
    "\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "# Create models dir\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
    "\n",
    "# COPY model files\n",
    "COPY model ${MODEL_BASE_PATH}/${MODEL_NAME}/1/\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/predictor.sh \\\n",
    "&& chmod +x /usr/bin/predictor.sh\n",
    "\n",
    "# REST API port\n",
    "EXPOSE 8501\n",
    "\n",
    "# Remove entrypoint from parent image\n",
    "ENTRYPOINT []\n",
    "\n",
    "CMD [\"/usr/bin/predictor.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "0fc60813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dockerfile_path}\n",
    "\n",
    "FROM tensorflow/serving:2.4.0\n",
    "\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
    "\n",
    "# copy the model file\n",
    "COPY model ${MODEL_BASE_PATH}/${MODEL_NAME}/1/\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=5000 --rest_api_port=8080 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/predictor.sh \\\n",
    "&& chmod +x /usr/bin/predictor.sh\n",
    "\n",
    "EXPOSE 5000\n",
    "EXPOSE 8080\n",
    "\n",
    "# Remove entrypoint from parent image\n",
    "ENTRYPOINT []\n",
    "\n",
    "CMD [\"/usr/bin/predictor.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9857b63",
   "metadata": {},
   "source": [
    "### Build serving image with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "8e555889",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SERVING_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_chicago_taxi_clsfr_serving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "d35e799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 693.6 KiB before compression.\n",
      "Uploading tarball of [predictor] to [gs://rt-vertex-sandbox_cloudbuild/source/1624384152.243359-52d98bcae9f7444dae7320188c8f8266.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rt-vertex-sandbox/locations/global/builds/6b34f51f-d318-4f34-ac7e-11166b30a3bd].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6b34f51f-d318-4f34-ac7e-11166b30a3bd?project=437422844431].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6b34f51f-d318-4f34-ac7e-11166b30a3bd\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rt-vertex-sandbox_cloudbuild/source/1624384152.243359-52d98bcae9f7444dae7320188c8f8266.tgz#1624384152590927\n",
      "Copying gs://rt-vertex-sandbox_cloudbuild/source/1624384152.243359-52d98bcae9f7444dae7320188c8f8266.tgz#1624384152590927...\n",
      "/ [1 files][144.5 KiB/144.5 KiB]                                                \n",
      "Operation completed over 1 objects/144.5 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  715.8kB\n",
      "Step 1/10 : FROM tensorflow/serving:2.4.0\n",
      "2.4.0: Pulling from tensorflow/serving\n",
      "f22ccc0b8772: Pulling fs layer\n",
      "3cf8fb62ba5f: Pulling fs layer\n",
      "e80c964ece6a: Pulling fs layer\n",
      "050093e6aebc: Pulling fs layer\n",
      "5a8c29228199: Pulling fs layer\n",
      "050093e6aebc: Waiting\n",
      "b80e0bd85e87: Pulling fs layer\n",
      "41291543e2b3: Pulling fs layer\n",
      "5a8c29228199: Waiting\n",
      "b80e0bd85e87: Waiting\n",
      "41291543e2b3: Waiting\n",
      "3cf8fb62ba5f: Verifying Checksum\n",
      "3cf8fb62ba5f: Download complete\n",
      "e80c964ece6a: Verifying Checksum\n",
      "e80c964ece6a: Download complete\n",
      "f22ccc0b8772: Verifying Checksum\n",
      "f22ccc0b8772: Download complete\n",
      "050093e6aebc: Verifying Checksum\n",
      "050093e6aebc: Download complete\n",
      "b80e0bd85e87: Verifying Checksum\n",
      "b80e0bd85e87: Download complete\n",
      "41291543e2b3: Verifying Checksum\n",
      "41291543e2b3: Download complete\n",
      "5a8c29228199: Verifying Checksum\n",
      "5a8c29228199: Download complete\n",
      "f22ccc0b8772: Pull complete\n",
      "3cf8fb62ba5f: Pull complete\n",
      "e80c964ece6a: Pull complete\n",
      "050093e6aebc: Pull complete\n",
      "5a8c29228199: Pull complete\n",
      "b80e0bd85e87: Pull complete\n",
      "41291543e2b3: Pull complete\n",
      "Digest: sha256:abc700ca693a8988a8b4e9a44686bb59e2902d06155eade7c5be659f3b4a806b\n",
      "Status: Downloaded newer image for tensorflow/serving:2.4.0\n",
      " ---> ffd2e2a4853e\n",
      "Step 2/10 : ENV MODEL_BASE_PATH=/models\n",
      " ---> Running in c8457fe05a04\n",
      "Removing intermediate container c8457fe05a04\n",
      " ---> 5bb5b310d0f6\n",
      "Step 3/10 : ENV MODEL_NAME=model\n",
      " ---> Running in d8f4ee8f8c8b\n",
      "Removing intermediate container d8f4ee8f8c8b\n",
      " ---> 59e93f051988\n",
      "Step 4/10 : RUN mkdir -p ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
      " ---> Running in bf55bb5109f7\n",
      "Removing intermediate container bf55bb5109f7\n",
      " ---> e89cea7b53f9\n",
      "Step 5/10 : COPY model ${MODEL_BASE_PATH}/${MODEL_NAME}/1/\n",
      " ---> df3d20ec961c\n",
      "Step 6/10 : RUN echo '#!/bin/bash \\n\\ntensorflow_model_server --port=5000 --rest_api_port=8080 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"' > /usr/bin/predictor.sh && chmod +x /usr/bin/predictor.sh\n",
      " ---> Running in 5eb9c4014f2a\n",
      "Removing intermediate container 5eb9c4014f2a\n",
      " ---> e4bc8448fac9\n",
      "Step 7/10 : EXPOSE 5000\n",
      " ---> Running in 0d07179462fc\n",
      "Removing intermediate container 0d07179462fc\n",
      " ---> fe5294e095eb\n",
      "Step 8/10 : EXPOSE 8080\n",
      " ---> Running in 083c33c34d8d\n",
      "Removing intermediate container 083c33c34d8d\n",
      " ---> 0826e8462cf3\n",
      "Step 9/10 : ENTRYPOINT []\n",
      " ---> Running in 8ef2599ec05e\n",
      "Removing intermediate container 8ef2599ec05e\n",
      " ---> 7edfa0512a76\n",
      "Step 10/10 : CMD [\"/usr/bin/predictor.sh\"]\n",
      " ---> Running in 47acf0989b08\n",
      "Removing intermediate container 47acf0989b08\n",
      " ---> 7c9d617a3d9c\n",
      "Successfully built 7c9d617a3d9c\n",
      "Successfully tagged gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_serving:latest\n",
      "PUSH\n",
      "Pushing gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_serving\n",
      "The push refers to repository [gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_serving]\n",
      "54d5b58c4707: Preparing\n",
      "4d3e99b8dad0: Preparing\n",
      "8b1db6910018: Preparing\n",
      "1974fcac8b60: Preparing\n",
      "fcbb0b09f809: Preparing\n",
      "e344483d680a: Preparing\n",
      "781dfadfb6ca: Preparing\n",
      "fe6d8881187d: Preparing\n",
      "23135df75b44: Preparing\n",
      "b43408d5f11b: Preparing\n",
      "e344483d680a: Waiting\n",
      "781dfadfb6ca: Waiting\n",
      "fe6d8881187d: Waiting\n",
      "23135df75b44: Waiting\n",
      "b43408d5f11b: Waiting\n",
      "1974fcac8b60: Layer already exists\n",
      "fcbb0b09f809: Layer already exists\n",
      "e344483d680a: Layer already exists\n",
      "781dfadfb6ca: Layer already exists\n",
      "fe6d8881187d: Layer already exists\n",
      "23135df75b44: Layer already exists\n",
      "b43408d5f11b: Layer already exists\n",
      "8b1db6910018: Pushed\n",
      "54d5b58c4707: Pushed\n",
      "4d3e99b8dad0: Pushed\n",
      "latest: digest: sha256:c116bd97a812a4ec1ad527c886f7e15b621c2b935def6481c051145f740ba853 size: 2404\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                              STATUS\n",
      "6b34f51f-d318-4f34-ac7e-11166b30a3bd  2021-06-22T17:49:12+00:00  26S       gs://rt-vertex-sandbox_cloudbuild/source/1624384152.243359-52d98bcae9f7444dae7320188c8f8266.tgz  gcr.io/rt-vertex-sandbox/rtvw_chicago_taxi_clsfr_serving (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {CUSTOM_SERVING_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f5124",
   "metadata": {},
   "source": [
    "#### Testing predictions locally [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f70f5",
   "metadata": {},
   "source": [
    "Run below commands on Cloud Shell locally to test predictions. You cannot run them on Vertex Notebooks since port 8080 is already in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a43d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local-taxi-tip-clsfr\n",
    "!docker pull {CUSTOM_SERVING_IMAGE}\n",
    "!docker run -t -d --rm -p 8080:8080 \\\n",
    "    --name=local-taxi-tip-clsfr \\\n",
    "    {CUSTOM_SERVING_IMAGE}\n",
    "!docker container ls\n",
    "!sleep 10\n",
    "!curl http://0.0.0.0:8080/v1/models/model\n",
    "!curl -d @instances.json -X POST http://0.0.0.0:8080/v1/models/model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d880c",
   "metadata": {},
   "source": [
    "```\n",
    "$ curl http://0.0.0.0:8080/v1/models/model\n",
    "{\n",
    " \"model_version_status\": [\n",
    "  {\n",
    "   \"version\": \"1\",\n",
    "   \"state\": \"AVAILABLE\",\n",
    "   \"status\": {\n",
    "    \"error_code\": \"OK\",\n",
    "    \"error_message\": \"\"\n",
    "   }\n",
    "  }\n",
    " ]\n",
    "}\n",
    "$ curl -d @instances.json -X POST http://0.0.0.0:8080/v1/models/model:predict\n",
    "{\n",
    "    \"predictions\": [[1.11188579]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c1fc6",
   "metadata": {},
   "source": [
    "### Deploy Model Resource with Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "124553b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/437422844431/locations/us-central1/models/7398015608979193856/operations/3061511237582979072\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/437422844431/locations/us-central1/models/7398015608979193856\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/437422844431/locations/us-central1/models/7398015608979193856')\n"
     ]
    }
   ],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Custom Serving'\n",
    "description = 'Chicago Taxi Tip TensorFlow classifier with TF Serving'\n",
    "\n",
    "custom_model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    serving_container_image_uri=CUSTOM_SERVING_IMAGE,\n",
    "    # serving_container_ports=[8501]\n",
    ")\n",
    "\n",
    "custom_model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa12cc5",
   "metadata": {},
   "source": [
    "### Create an Endpoint for Model with Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "04222cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/437422844431/locations/us-central1/endpoints/8940542461818699776/operations/1467236969493823488\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/437422844431/locations/us-central1/endpoints/8940542461818699776\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/437422844431/locations/us-central1/endpoints/8940542461818699776')\n"
     ]
    }
   ],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Endpoint Custom Serving'\n",
    "\n",
    "custom_endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb820f94",
   "metadata": {},
   "source": [
    "### Deploy the Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6b8d7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/437422844431/locations/us-central1/endpoints/8940542461818699776\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/437422844431/locations/us-central1/endpoints/8940542461818699776/operations/2178805710618361856\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=437422844431&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%228940542461818699776%22%0Aresource.labels.location%3D%22us-central1%22.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-7e9996b22598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmachine_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         traffic_percentage=traffic_percentage)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m         )\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         )\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\u001b[0m\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def undeploy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=437422844431&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%228940542461818699776%22%0Aresource.labels.location%3D%22us-central1%22."
     ]
    }
   ],
   "source": [
    "deployed_model_display_name = f'{PREFIX}-taxi-v1'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "custom_endpoint = custom_model.deploy(\n",
    "        endpoint=custom_endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0c263",
   "metadata": {},
   "source": [
    "## Invoking the Deployed Model using Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = f'display_name=\"{PREFIX} Chicago Taxi Tip Classifier Endpoint Custom Serving\"'\n",
    "\n",
    "for endpoint_info in vertex_ai.Endpoint.list(filter=filter):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "custom_endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ec947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = custom_endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee825d",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28306d7",
   "metadata": {},
   "source": [
    "### Undeploy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "04b83b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"7576022143469617152\"\n",
       "model: \"projects/437422844431/locations/us-central1/models/174804756630339584\"\n",
       "display_name: \"rtvw-taxi-v1\"\n",
       "create_time {\n",
       "  seconds: 1624249495\n",
       "  nanos: 154359000\n",
       "}\n",
       "dedicated_resources {\n",
       "  machine_spec {\n",
       "    machine_type: \"n1-standard-4\"\n",
       "  }\n",
       "  min_replica_count: 1\n",
       "  max_replica_count: 1\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44067f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dce22",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa9dbb",
   "metadata": {},
   "source": [
    "### Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5d112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
