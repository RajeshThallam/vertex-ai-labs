{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model on Vertex AI based on new model artifacts in GCS\n",
    "\n",
    "Customer is looking to upload Tensorflow models from Cloud Storage bucket to Vertex Predictions. Customer's customer uploads model artifacts from trained models to Cloud Storage bucket. When there is a new upload, trigger [model upload and deployment](https://cloud.google.com/vertex-ai/docs/general/import-model) process to Vertex Predictions.\n",
    "\n",
    "![Event based model deployment](./images/event_based_model_deployment.png)\n",
    "\n",
    "High level steps\n",
    "\n",
    "* Custom uploads trained model artifacts to Cloud Storage bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "Once you've installed the required packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import google.auth\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create event trigger\n",
    "- Create Cloud Run event target\n",
    "  - read model artifact location in GCS\n",
    "  - triggers Cloud Build to create custom container image\n",
    "    - copy model artifacts to Cloud Build local from GCS\n",
    "    - copy Dockerfile to Cloud Build local from GCS\n",
    "    - Build image using Dockerfile and `docker build -t`\n",
    "      - Copy model artifacts to the image\n",
    "      - Expose custom ports\n",
    "      - Start TF Serving HTTP server\n",
    "  - Upload model resource with custom container image uri\n",
    "  - Create endpoint\n",
    "  - Deploy model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  rthallam-demo-project\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # <---CHANGE THIS TO YOUR PROJECT\n",
    "\n",
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID using google.auth\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import google.auth\n",
    "\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "# validate PROJECT_ID\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    print(\n",
    "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Event Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the configuration variables used in this quickstart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [run/region].\n",
      "Updated property [run/platform].\n",
      "Updated property [eventarc/location].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $REGION\n",
    "\n",
    "REGION=$1\n",
    "\n",
    "gcloud config set run/region $REGION\n",
    "gcloud config set run/platform managed\n",
    "gcloud config set eventarc/location $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Enable Cloud Audit Logs Admin Read, Data Read, and Data Write Log Types in Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read your project's IAM policy and store it in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID > ./tmp/policy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edit your policy in `./tmp/policy.yaml`, adding or changing only the Data Access audit logs configuration for Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"\"\"\n",
    "auditConfigs:\n",
    "- auditLogConfigs:\n",
    "  - logType: ADMIN_READ\n",
    "  - logType: DATA_WRITE\n",
    "  - logType: DATA_READ\n",
    "  service: storage.googleapis.com\"\"\" | sed -i -e '/^auditConfigs:/{r /dev/stdin' -e 'd;}' ./tmp/policy.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set your new IAM policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.projects.set-iam-policy) User [560224572293-compute@developer.gserviceaccount.com] does not have permission to access projects instance [rthallam-demo-project:setIamPolicy] (or it may not exist): Policy update access denied.\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects set-iam-policy $PROJECT_ID ./tmp/policy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "auditConfigs:\n",
      "- auditLogConfigs:\n",
      "  - logType: ADMIN_READ\n",
      "  - logType: DATA_WRITE\n",
      "  - logType: DATA_READ\n",
      "  service: storage.googleapis.com\n",
      "- auditLogConfigs:\n",
      "  - logType: ADMIN_READ\n",
      "  - logType: DATA_READ\n",
      "  - logType: DATA_WRITE\n",
      "  service: accessapproval.googleapis.com\n",
      "- auditLogConfigs:\n",
      "  - logType: ADMIN_READ\n",
      "  - logType: DATA_READ\n",
      "  - logType: DATA_WRITE\n",
      "  service: dialogflow.googleapis.com\n",
      "- auditLogConfigs:\n",
      "  - logType: ADMIN_READ\n",
      "  - logType: DATA_READ\n",
      "  - logType: DATA_WRITE\n",
      "  service: iam.googleapis.com\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:rt-cloudai-services@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@gcp-sa-staging-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-gae-service.iam.gserviceaccount.com\n",
      "  role: roles/appengine.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:automl-demo@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:rt-cloudai-services@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/automl.admin\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/automl.predictor\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-automl.iam.gserviceaccount.com\n",
      "  role: roles/automl.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-recommendationengine.iam.gserviceaccount.com\n",
      "  role: roles/automlrecommendations.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com\n",
      "  role: roles/bigquerydatatransfer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-cloudasset.iam.gserviceaccount.com\n",
      "  role: roles/cloudasset.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:560224572293@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcf-admin-robot.iam.gserviceaccount.com\n",
      "  role: roles/cloudfunctions.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-cloudscheduler.iam.gserviceaccount.com\n",
      "  role: roles/cloudscheduler.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/cloudsql.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-tpu.iam.gserviceaccount.com\n",
      "  role: roles/cloudtpu.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@cloudcomposer-accounts.iam.gserviceaccount.com\n",
      "  role: roles/composer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:f9-sector-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/compute.admin\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/compute.networkAdmin\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@dataproc-accounts.iam.gserviceaccount.com\n",
      "  role: roles/container.admin\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/container.clusterAdmin\n",
      "- members:\n",
      "  - serviceAccount:560224572293@cloudbuild.gserviceaccount.com\n",
      "  role: roles/container.developer\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:560224572293-compute@developer.gserviceaccount.com\n",
      "  role: roles/datafusion.runner\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-datafusion.iam.gserviceaccount.com\n",
      "  role: roles/datafusion.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-datalabeling.iam.gserviceaccount.com\n",
      "  role: roles/datalabeling.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@trifacta-gcloud-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataprep.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@dataproc-accounts.iam.gserviceaccount.com\n",
      "  role: roles/dataproc.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@dataproc-accounts.iam.gserviceaccount.com\n",
      "  role: roles/dataproc.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/dialogflow.admin\n",
      "- members:\n",
      "  - serviceAccount:dialogflow-jkpwdb@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:dialogflow-ktasoc@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:dialogflow-pnxvgy@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/dialogflow.client\n",
      "- condition:\n",
      "    expression: resource.name.startsWith(\"projects/rthallam-demo-project/locations/global/agents/b7f15541-fe94-40d4-b5d0-08a65101a7ed\")\n",
      "      || resource.name.startsWith(\"projects/rthallam-demo-project/locations/global/agents/-\")\n",
      "      || resource.name.startsWith(\"projects/rthallam-demo-project/locations/global/operations\")\n",
      "      || resource.name.startsWith(\"projects/rthallam-demo-project/locations/us/operations\")\n",
      "      || resource.name == \"projects/rthallam-demo-project\"\n",
      "    title: For Dialogflow Agent b7f15541-fe94-40d4-b5d0-08a65101a7ed\n",
      "  members:\n",
      "  - user:rthallam@altostrat.com\n",
      "  role: roles/dialogflow.reader\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-dialogflow.iam.gserviceaccount.com\n",
      "  role: roles/dialogflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-preprod-alpha-dai-core.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@gcp-sa-prod-dai-core.iam.gserviceaccount.com\n",
      "  role: roles/documentaicore.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:560224572293-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:560224572293@cloudservices.gserviceaccount.com\n",
      "  - serviceAccount:rthallam-demo-project@appspot.gserviceaccount.com\n",
      "  - serviceAccount:service-560224572293@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-eventarc.iam.gserviceaccount.com\n",
      "  role: roles/eventarc.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-service-account@firebase-sa-management.iam.gserviceaccount.com\n",
      "  role: roles/firebase.managementServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-adminsdk-n6loc@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/firebase.sdkAdminServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@firebase-rules.iam.gserviceaccount.com\n",
      "  role: roles/firebaserules.system\n",
      "- members:\n",
      "  - serviceAccount:fhir-healthcareapi@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.fhirResourceEditor\n",
      "- members:\n",
      "  - serviceAccount:fhir-healthcareapi@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.fhirResourceReader\n",
      "- members:\n",
      "  - serviceAccount:fhir-healthcareapi@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.fhirStoreAdmin\n",
      "- members:\n",
      "  - serviceAccount:fhir-healthcareapi@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.fhirStoreViewer\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-healthcare.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-adminsdk-n6loc@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountTokenCreator\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-gke-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/logging.logWriter\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:rt-cloudai-services@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/ml.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@cloud-ml.google.com.iam.gserviceaccount.com\n",
      "  role: roles/ml.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-gke-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/monitoring.metricWriter\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-gke-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/monitoring.viewer\n",
      "- members:\n",
      "  - serviceAccount:rt-cloudai-services@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - user:rthallam@google.com\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:ccaideployer@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/redis.admin\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@cloud-redis.iam.gserviceaccount.com\n",
      "  role: roles/redis.serviceAgent\n",
      "- members:\n",
      "  - user:rthallam@google.com\n",
      "  role: roles/resourcemanager.projectMover\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@serverless-robot-prod.iam.gserviceaccount.com\n",
      "  role: roles/run.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@sourcerepo-service-accounts.iam.gserviceaccount.com\n",
      "  role: roles/sourcerepo.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-gke-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/stackdriver.resourceMetadata.writer\n",
      "- members:\n",
      "  - serviceAccount:automl-demo@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:mlops-rt-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:rt-cloudai-services@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-437422844431@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "- members:\n",
      "  - serviceAccount:automl-demo@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectAdmin\n",
      "- members:\n",
      "  - serviceAccount:mlops-rt-gke-sa@rthallam-demo-project.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectViewer\n",
      "- members:\n",
      "  - serviceAccount:service-560224572293@cloud-tpu.iam.gserviceaccount.com\n",
      "  role: roles/tpu.serviceAgent\n",
      "etag: BwXJKafNg7I=\n",
      "version: 3\n"
     ]
    }
   ],
   "source": [
    "!cat ./tmp/policy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# from pathlib import Path\n",
    "# import subprocess\n",
    "import google.auth\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "credentials, project_id = google.auth.default()\n",
    "client = cloudbuild_v1.services.cloud_build.CloudBuildClient()\n",
    "\n",
    "build_version = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "dockerfile_path = os.path.join(cifar_config.path, 'Dockerfile')\n",
    "gs_dockerfile_path = dockerfile_path.replace(\"/gcs/\", \"gs://\")\n",
    "config_prop_path = os.path.join(cifar_config.path, 'config.properties')\n",
    "gs_config_prop_path = config_prop_path.replace(\"/gcs/\", \"gs://\")\n",
    "\n",
    "export_path = f\"{cifar_mar.path}/model-store\"\n",
    "model_path = os.path.join(export_path, mar_model_name)\n",
    "gs_model_path = model_path.replace(\"/gcs/\", \"gs://\")\n",
    "logging.warning(\"gs_model_path: %s\", gs_model_path)\n",
    "\n",
    "image_uri = f\"gcr.io/{project}/pytorch_train_cifar10:{build_version}\"\n",
    "logging.info(\"image uri: %s\", image_uri)\n",
    "\n",
    "build = cloudbuild_v1.Build(\n",
    "    images=[image_uri]\n",
    ")\n",
    "build.steps = [\n",
    "    {\n",
    "        \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "        \"args\": [\n",
    "            \"cp\",\n",
    "            gs_config_prop_path,\n",
    "            \"config.properties\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "        \"args\": [\"cp\", f\"{gs_model_path}\", f\"{mar_model_name}\"],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "        \"args\": [\n",
    "            \"cp\",\n",
    "            gs_dockerfile_path,\n",
    "            \"Dockerfile\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "      \"name\": 'gcr.io/cloud-builders/docker',\n",
    "      \"args\": [ 'build', '-t', image_uri, '.' ]\n",
    "    }\n",
    "]\n",
    "operation = client.create_build(project_id=project, build=build)\n",
    "print(\"IN PROGRESS:\")\n",
    "print(operation.metadata)\n",
    "\n",
    "result = operation.result()\n",
    "# Print the completed status\n",
    "print(\"RESULT:\", result.status)\n",
    "return(image_uri,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2020 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "BQ_LOCATION = 'US'\n",
    "SAMPLE_SIZE = 500000\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a BQ dataset to host the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(location=BQ_LOCATION)\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training, validation, and test splits tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script_template = '''\n",
    "CREATE TEMP TABLE features \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'TEST'\n",
    "          WHEN 8 THEN 'VALIDATE'\n",
    "          ELSE 'TRAIN' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TRAIN';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='VALIDATE';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TEST';\n",
    "\n",
    "DROP TABLE features;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = sql_script_template.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TRAIN_SPLIT', BQ_TRAIN_SPLIT_NAME).replace(\n",
    "    '@VALIDATE_SPLIT', BQ_VALID_SPLIT_NAME).replace(\n",
    "    '@TEST_SPLIT', BQ_TEST_SPLIT_NAME).replace(\n",
    "    '@YEAR', str(YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE))\n",
    "\n",
    "job = client.query(sql_script)\n",
    "job.result()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM `{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}`\n",
    "'''\n",
    "\n",
    "data = client.query(sql_script).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training pipeline](../images/custom-training-on-vertex-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    history = model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK and Set an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define experiment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"chicago-taxi-tips-classifier\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If EXPERIMENT_NAME is not set, set a default one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPERIMENT_NAME == \"\" or EXPERIMENT_NAME is None:\n",
    "    EXPERIMENT_NAME = \"my-experiment-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-{TIMESTAMP}\"\n",
    "base_output_dir = f'{STAGING_BUCKET}/jobs/{job_name}'\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    f'--epochs={epochs}', \n",
    "    f'--per_replica_batch_size={batch_size}',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "\n",
    "vertex_ai.start_run(\"custom-training-run-1\")  # Change this to your desired run name\n",
    "parameters = {\"epochs\": 2, \n",
    "              \"batch_size\": 128,\n",
    "              \"training_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "              \"validation_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}'\n",
    "             }\n",
    "vertex_ai.log_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Job Name: {job.display_name}\")\n",
    "print(f\"Job Resource Name: {job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model artifacts in GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $base_output_dir/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model artifacts to be deployed to Vertex Model Resource later in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container\n",
    "\n",
    "![training-with-custom-containers-on-vertex-training](../images/training-with-custom-containers-on-vertex-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_chicago_taxi_clsfr_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                f'--epochs={epochs}', \n",
    "                f'--per_replica_batch_size={batch_size}',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")\n",
    "\n",
    "vertex_ai.start_run(\"custom-training-run-2\")  # Change this to your desired run name\n",
    "parameters = {\"epochs\": epochs, \n",
    "              \"batch_size\": batch_size,\n",
    "              \"training_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "              \"validation_table\": f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}'\n",
    "             }\n",
    "vertex_ai.log_params(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Job Name: {job.display_name}\")\n",
    "print(f\"Job Resource Name: {job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {STAGING_BUCKET}/{job_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and submit a Hyperparameter Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# The spec of the worker pools including machine type and Docker image\n",
    "# Be sure to replace IMAGE_URI with the path to your Docker image in GCR\n",
    "worker_pool_specs = [{\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": \"n1-standard-4\",\n",
    "      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "      #  \"accelerator_count\": 1,\n",
    "    },\n",
    "    \"replica_count\": 1,\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": TRAIN_IMAGE,\n",
    "        \"args\": [\n",
    "            f'--epochs={epochs}', \n",
    "            f'--per_replica_batch_size={batch_size}',\n",
    "            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "        ],\n",
    "    },\n",
    "}]\n",
    "\n",
    "# Dicionary representing metrics to optimize.\n",
    "# The dictionary key is the metric_id, which is reported by your training job,\n",
    "# And the dictionary value is the optimization goal of the metric.\n",
    "metric_spec={'val_accuracy':'maximize'}\n",
    "\n",
    "# Dictionary representing parameters to optimize.\n",
    "# The dictionary key is the parameter_id, which is passed into your training\n",
    "# job as a command line argument,\n",
    "# And the dictionary value is the parameter specification of the metric.\n",
    "parameter_spec = {\n",
    "    \"units\": hpt.DiscreteParameterSpec(values=[32, 64], scale=None),\n",
    "    \"dropout_ratio\": hpt.DoubleParameterSpec(min=0.4, max=0.6, scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure custom job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"chicago-taxi-clsfr-custom-hp-{TIMESTAMP}\"\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and run Hyperparameter Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_job_name = f\"chicago-taxi-clsfr-hptuning-{TIMESTAMP}\"\n",
    "\n",
    "hp_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=hp_job_name,\n",
    "    custom_job=job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=3,\n",
    "    parallel_trial_count=3,\n",
    "    max_failed_trial_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Job Name: {hp_job.display_name}\")\n",
    "print(f\"Job Resource Name: {hp_job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {hp_job._dashboard_uri()}\")\n",
    "\n",
    "print(f\"Job state {hp_job.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if hp_job.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", hp_job.state)\n",
    "        if (hp_job.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           hp_job.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_hp_job_results = pd.DataFrame([MessageToDict(trial.__class__.pb(trial)) for trial in hp_job.trials])\n",
    "df_hp_job_results[\"parameters\"] = df_hp_job_results.parameters.apply(lambda x: {item[\"parameterId\"]:item[\"value\"] for item in x})\n",
    "df_hp_job_results[\"metrics\"] = df_hp_job_results.finalMeasurement.apply(lambda x: x[\"metrics\"][0][\"value\"])\n",
    "df_hp_job_results.drop(\"finalMeasurement\", axis=1, inplace=True)\n",
    "df_hp_job_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = df_hp_job_results.iloc[[df_hp_job_results[\"metrics\"].idxmax()]]\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f'{STAGING_BUCKET}/{job_name}'\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id.values[0])\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a model to Vertex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = f'{best_model_dir}/model'\n",
    "\n",
    "!saved_model_cli show --dir {saved_model_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the model using pre-built serving container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier'\n",
    "description = 'Chicago Taxi Tip TensorFlow classifier'\n",
    "serving_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest'\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    artifact_uri=saved_model_path,\n",
    "    serving_container_image_uri=serving_image_uri\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Endpoint'\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_display_name = f'{PREFIX}-taxi-v1'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the deployed model using Vertex SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = f'display_name=\"{PREFIX} Chicago Taxi Tip Classifier Endpoint\"'\n",
    "\n",
    "for endpoint_info in vertex_ai.Endpoint.list(filter=filter):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model using custom serving container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to use a custom serving container with Vertex Predictions for any of the following reasons:\n",
    "\n",
    "- To serve predictions from an ML model trained using a framework other than TensorFlow, scikit-learn, or XGBoost\n",
    "- To preprocess prediction requests or postprocess the predictions generated by your model\n",
    "- To run a prediction server written in a programming language of your choice\n",
    "- To install dependencies that you want to use to customize prediction\n",
    "\n",
    "When you use a custom container, Vertex AI runs a Docker container of your choice on each prediction node. The [container image requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) must meet to be compatible with Vertex AI.\n",
    "\n",
    "Following diagram shows the steps to deploy a model on vertex AI with custom container.\n",
    "\n",
    "![serving with custom container on Vertex AI](../images/serving-with-custom-containers-on-vertex-predictions.png)\n",
    "\n",
    "Let's deploy the previously trained Tensorflow/Keras model on Vertex AI with Tensorflow Serving custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile with serving container image\n",
    "\n",
    "Dockerfile image consist of following steps\n",
    "\n",
    "1. Create image from base Tensorflow serving image `tensorflow/serving:2.4`\n",
    "2. Copy model artifacts (Tensorflow savedModel) generated from the training job (available in GCS bucket) in the image\n",
    "3. Run Tensorflow Server process in the backend listening to prediction requests on port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'predictor'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "dockerfile_path = os.path.join(SCRIPT_FOLDER, 'Dockerfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Model Artifacts from GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saved model artifacts from the training job located at {saved_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = 'model'\n",
    "if tf.io.gfile.exists(MODEL_FOLDER):\n",
    "    tf.io.gfile.rmtree(MODEL_FOLDER)\n",
    "tf.io.gfile.mkdir(f\"{SCRIPT_FOLDER}/{MODEL_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{SCRIPT_FOLDER}/{MODEL_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://rtvw-rt-vertex-sandbox-bucket/chicago-taxi-clsfr-custom-hp-20210621_002500/1/model/* $SCRIPT_FOLDER/$MODEL_FOLDER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dockerfile_path}\n",
    "\n",
    "FROM tensorflow/serving\n",
    "\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "# Create models dir\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
    "\n",
    "# COPY model files\n",
    "COPY model ${MODEL_BASE_PATH}/${MODEL_NAME}/1/\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/predictor.sh \\\n",
    "&& chmod +x /usr/bin/predictor.sh\n",
    "\n",
    "# REST API port\n",
    "EXPOSE 8501\n",
    "\n",
    "# Remove entrypoint from parent image\n",
    "ENTRYPOINT []\n",
    "\n",
    "CMD [\"/usr/bin/predictor.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {dockerfile_path}\n",
    "\n",
    "FROM tensorflow/serving:2.4.0\n",
    "\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
    "\n",
    "# copy the model file\n",
    "COPY model ${MODEL_BASE_PATH}/${MODEL_NAME}/1/\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=5000 --rest_api_port=8080 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/predictor.sh \\\n",
    "&& chmod +x /usr/bin/predictor.sh\n",
    "\n",
    "EXPOSE 5000\n",
    "EXPOSE 8080\n",
    "\n",
    "# Remove entrypoint from parent image\n",
    "ENTRYPOINT []\n",
    "\n",
    "CMD [\"/usr/bin/predictor.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build serving image with Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SERVING_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_chicago_taxi_clsfr_serving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{dockerfile_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag {CUSTOM_SERVING_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing predictions locally [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below commands on Cloud Shell locally to test predictions. You cannot run them on Vertex Notebooks since port 8080 is already in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local-taxi-tip-clsfr\n",
    "!docker pull {CUSTOM_SERVING_IMAGE}\n",
    "!docker run -t -d --rm -p 8080:8080 \\\n",
    "    --name=local-taxi-tip-clsfr \\\n",
    "    {CUSTOM_SERVING_IMAGE}\n",
    "!docker container ls\n",
    "!sleep 10\n",
    "!curl http://0.0.0.0:8080/v1/models/model\n",
    "!curl -d @instances.json -X POST http://0.0.0.0:8080/v1/models/model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ curl http://0.0.0.0:8080/v1/models/model\n",
    "{\n",
    " \"model_version_status\": [\n",
    "  {\n",
    "   \"version\": \"1\",\n",
    "   \"state\": \"AVAILABLE\",\n",
    "   \"status\": {\n",
    "    \"error_code\": \"OK\",\n",
    "    \"error_message\": \"\"\n",
    "   }\n",
    "  }\n",
    " ]\n",
    "}\n",
    "$ curl -d @instances.json -X POST http://0.0.0.0:8080/v1/models/model:predict\n",
    "{\n",
    "    \"predictions\": [[1.11188579]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model Resource with Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Custom Serving - debug'\n",
    "description = 'Chicago Taxi Tip TensorFlow classifier with TF Serving'\n",
    "\n",
    "MODEL_BASE_PATH = \"models\"\n",
    "MODEL_NAME = \"model\"\n",
    "health_route = f\"/v1/{MODEL_BASE_PATH}/{MODEL_NAME}\"\n",
    "predict_route = f\"/v1/{MODEL_BASE_PATH}/{MODEL_NAME}:predict\"\n",
    "\n",
    "custom_model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    serving_container_image_uri=CUSTOM_SERVING_IMAGE,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route\n",
    "    # serving_container_ports=[8501]\n",
    ")\n",
    "\n",
    "custom_model.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint for Model with Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = f'{PREFIX} Chicago Taxi Tip Classifier Endpoint Custom Serving - debug'\n",
    "\n",
    "custom_endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model to Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOTE: When deploying the model with custom container (with TF Serving) to Vertex Endpoints, it gets stuck. This is an open issue we are tracking.**\n",
    "\n",
    "Error you may encounter\n",
    "\n",
    "```\n",
    "FailedPrecondition: 400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at https://console.cloud.google.com/logs/viewer?...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployed_model_display_name = f'{PREFIX}-taxi-v1-debug'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "custom_endpoint = custom_model.deploy(\n",
    "        endpoint=custom_endpoint,\n",
    "        deployed_model_display_name=custom_model.display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the Deployed Model using Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = f'display_name=\"{PREFIX} Chicago Taxi Tip Classifier Endpoint Custom Serving - debug\"'\n",
    "\n",
    "for endpoint_info in vertex_ai.Endpoint.list(filter=filter):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "custom_endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_endpoint = vertex_ai.Endpoint('projects/437422844431/locations/us-central1/endpoints/3542837570926804992')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = custom_endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undeploy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
