{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHLV0D7Y5jtU"
   },
   "source": [
    "# BERT fine-tuning using Vertex Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use [Vertex AI](https://cloud.google.com/vertex-ai) to run TensorFlow 2.x distributed training with GPUs. Both single node and multi-worker scenarios are covered.\n",
    "\n",
    "The ML scenario is BERT fine-tuning. You will use the text IMDB movie reviews database and the pre-trained BERT model from the [TensorFlow Hub](https://www.tensorflow.org/hub) to develop a text classification model for sentiment analysis.\n",
    "\n",
    "\n",
    "There are three types of AI Platform resources you can use to train custom models on AI Platform:\n",
    "\n",
    "- [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job)\n",
    "- [Hyperparameter tuning jobs](https://cloud.google.com/ai-platform-unified/docs/training/using-hyperparameter-tuning)\n",
    "- [Training pipelines](https://cloud.google.com/ai-platform-unified/docs/training/create-training-pipeline)\n",
    "\n",
    "![Training pipeline](../images/custom-training-on-vertex-ai.png)\n",
    "\n",
    "This sample focuses on [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job) with [custom training containers](https://cloud.google.com/ai-platform-unified/docs/training/containers-overview).\n",
    "\n",
    "In the notebook, you will go through the following steps:\n",
    "\n",
    "- Converting the text IMDB database to the TFRecords format\n",
    "- Developing a custom training container\n",
    "- Configuring, submitting, and monitoring single node and multi-worker Custom training jobs\n",
    "\n",
    "\n",
    "### About BERT\n",
    "\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-aiplatform\n",
    "!pip install --user kfp\n",
    "!pip install --user google-cloud-pipeline-components\n",
    "!pip install --user google-cloud-bigquery-datatransfer\n",
    "!pip install --user tf-models-official==2.4.0 tensorflow-text==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "Once you've installed the required packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import google.auth\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT = rt-vertex-sandbox\n",
      "STAGING_BUCKET = gs://rtvw-rt-vertex-sandbox-bucket\n",
      "VERTEX_SA = training-sa@rt-vertex-sandbox.iam.gserviceaccount.com\n",
      "TENSORBOARD = projects/437422844431/locations/us-central1/tensorboards/6509988445736140800\n"
     ]
    }
   ],
   "source": [
    "creds, PROJECT = google.auth.default()\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'rtvw'            # <--- CHANGE THIS TO VARIABLE YOU SET UP DURING PREREQUISITES\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-{PROJECT}-bucket'\n",
    "BUCKET_NAME = STAGING_BUCKET\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "print(f\"PROJECT = {PROJECT}\")\n",
    "print(f\"STAGING_BUCKET = {STAGING_BUCKET}\")\n",
    "print(f\"VERTEX_SA = {VERTEX_SA}\")\n",
    "\n",
    "# get tensorboard instance\n",
    "shell_output = !gcloud beta ai tensorboards list --region $REGION --filter=displayName:$PREFIX-$REGION-tensorboard --format='value(name)' --quiet 2>/dev/null\n",
    "TENSORBOARD = shell_output[-1]\n",
    "print(f\"TENSORBOARD = {TENSORBOARD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "### Preparing data\n",
    "\n",
    "In this section, you will convert the original IMDB dataset that is in plain text into the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format. The TFRecord format is recommended for high performance input pipelines that are critical in large scale training scenarios like BERT pre-training and fine-tuning. The TFRecord format works well with the [tf.data API](https://www.tensorflow.org/guide/data) used to implement input pipelines in this sample.\n",
    "\n",
    "After the TFRecord files are created, you will copy them to a GCS storage bucket. In most distributed training scenarios, training data needs to be located in a shared storage location.\n",
    "\n",
    "#### Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "local_dir = os.path.expanduser('~')\n",
    "local_dir = f'{local_dir}/distributed-training/datasets'\n",
    "\n",
    "if tf.io.gfile.exists(local_dir):\n",
    "    tf.io.gfile.rmtree(local_dir)\n",
    "tf.io.gfile.makedirs(local_dir)\n",
    "\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "local_path = f'{local_dir}/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file(local_path, url,\n",
    "                                  untar=True, \n",
    "                                  cache_dir=local_dir,\n",
    "                                  cache_subdir='.'\n",
    "                                  )\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the IMDB dataset to TFRecords files\n",
    "\n",
    "##### Create training, validation and testing splits from IMDB text files\n",
    "\n",
    "The IMDB dataset has already been divided into train and test, but it lacks a validation set. We will create a validation set using an 80:20 split of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(train_dir, test_dir, val_split, seed):\n",
    "    \n",
    "    train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='training',\n",
    "        seed=seed)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    train_ds = train_ds.unbatch()\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='validation',\n",
    "        seed=seed).unbatch()\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        test_dir).unbatch()\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "val_split = 0.2\n",
    "test_dir = f'{dataset_dir}/test'\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = (\n",
    "    create_splits(train_dir, test_dir, val_split, seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect a couple of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label : 0 (neg)\n",
      "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_ds.take(2):\n",
    "    print(f'Review: {text.numpy()}')\n",
    "    label = label.numpy()\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare tf.Example serialization routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(text_fragment, label):\n",
    "    \"\"\"Serializes text fragment and label in tf.Example.\"\"\"\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    feature = {\n",
    "        'text_fragment': _bytes_feature(text_fragment),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "    \n",
    "def tf_serialize_example(text_fragment, label):\n",
    "  tf_string = tf.py_function(\n",
    "    serialize_example,\n",
    "    (text_fragment, label), \n",
    "    tf.string)      \n",
    "  return tf.reshape(tf_string, ()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_folder = '{}/tfrecords'.format(os.path.expanduser('~'))\n",
    "if tf.io.gfile.exists(tfrecords_folder):\n",
    "    tf.io.gfile.rmtree(tfrecords_folder)\n",
    "tf.io.gfile.makedirs(tfrecords_folder)\n",
    "\n",
    "filenames = ['train.tfrecords', 'valid.tfrecords', 'test.tfrecords']\n",
    "for file_name, dataset in zip(filenames, [train_ds, val_ds, test_ds]):\n",
    "    writer = tf.data.experimental.TFRecordWriter(os.path.join(tfrecords_folder, file_name))\n",
    "    writer.write(dataset.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double check that you can read the created TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\xcc\\x05\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\xb9\\x05\\n\\rtext_fragment\\x12\\xa7\\x05\\n\\xa4\\x05\\n\\xa1\\x05Another \"must have\" film. Henry Brandon is a favorite! I was so surprised when I learned years ago that he was from Germany because he sounds & looks so typically American! And wasn\\'t he great in \"The Searchers\" as Chief Scar??!! Another of my favorites, I have it & watch it over & again. Now if I could add this one to my collection, it would make my day! This is a great wildlife story & film for all ages. The scenery is so absolutely beautiful & the plight of the endangered snow leopards is told with such great emotion it will spark the interest in endangered species in anyone, especially children. If I could I would give a copy to all of my grands & great grands!', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\n\\xa6\\x08\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x93\\x08\\n\\rtext_fragment\\x12\\x81\\x08\\n\\xfe\\x07\\n\\xfb\\x07When you think \\'Oliver Stone\\' the movies that come to mind would be his biggest and most controversial ones like Platoon, JFK, Born On The Fourth Of July, or Natural Born Killers. Talk Radio usually doesn\\'t. It\\'s a pretty small movie, actually. More than half the movie takes place with Barry Champlain at his radio station talking into his mike. But believe me, this is one of Oliver Stone\\'s greatest movies and should NOT be missed.<br /><br />Above all things it\\'s a character study. Barry Champlain is a rude, self-destructive, risk-taking talk radio show host who says one too many things and starts to get in trouble with his boss, his lover(s), his fans, and even some Nazis. He doesn\\'t like his audience and callers and a lot of them don\\'t like him (eithor that or do like him, but have no idea why). But, at the end he says on his show: \"I guess we\\'re stuck with each other.\"<br /><br />See Talk Radio, even if you don\\'t like Oliver Stone movies. You might be surprised. I sure was.<br /><br />My Rating: 10/10', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for record in tf.data.TFRecordDataset([os.path.join(tfrecords_folder, file_name)]).take(2):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the created TFRecord files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/tfrecords/train.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 26.5 MiB/ 26.5 MiB]                                                \n",
      "Operation completed over 1 objects/26.5 MiB.                                     \n",
      "Copying file:///home/jupyter/tfrecords/valid.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  6.6 MiB/  6.6 MiB]                                                \n",
      "Operation completed over 1 objects/6.6 MiB.                                      \n",
      "Copying file:///home/jupyter/tfrecords/test.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 32.3 MiB/ 32.3 MiB]                                                \n",
      "Operation completed over 1 objects/32.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gcs_paths = [f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train',\n",
    "             f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid',\n",
    "             f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test']\n",
    "\n",
    "for filename, gcs_path in zip(filenames, gcs_paths):\n",
    "    local_file_path = os.path.join(tfrecords_folder, filename)\n",
    "    gcs_file_path = f'{gcs_path}/{filename}'\n",
    "    !gsutil cp {local_file_path} {gcs_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training container image\n",
    "\n",
    "\n",
    "There are two ways of packaging your training code for AI Platform Custom jobs. \n",
    "\n",
    "- **Use a Google Cloud prebuilt container**. If you use a prebuilt container, you will additionally specify a Python package to install into the container image. This Python package contains your code for training a custom model.\n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container needs to contain your code plus all the dependencies..\n",
    "\n",
    "In this sample, we are using a custom container.\n",
    "\n",
    "To create a custom training container you need to define a Python training module and package it in a container image together with all the required dependencies.\n",
    "\n",
    "We will use the standard [Deep Learning Containers](https://cloud.google.com/ai-platform/deep-learning-containers/docs) image as a base image for the custom traininer container image. Specifically we are going to use the `gcr.io/deeplearning-platform-release/tf2-gpu.2-4` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn-LlmWGxa35"
   },
   "source": [
    "\n",
    "### Create the training module\n",
    "\n",
    "A custom training image encapsulates you training code. You can structure your code in anyway you want as long as you can invoke it through a standard docker container interface. \n",
    "\n",
    "In this sample, the training code is encapsulated in a single Python module - `task.py`. The runtime parameters can be passed as command line arguments.  The below section summarizes key design decisions taken when designing the training regime.\n",
    "\n",
    "#### Model design\n",
    "\n",
    "This sample implements a simple classification model using pre-trained BERT components from TensorFlow Hub. Specifically a classic BERT architecture with L=12 hidden layers, a hidden size of H=768, and A=12 attention heads is used. This [TF Hub model](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) uses the implementatio of BERT from the [TensorFlow Model Garden repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert). \n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
    "\n",
    "The model implemented in the script embedds the preprocessing model from TF Hub as a Keras layer.\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), the model uses `tf.keras.losses.BinaryCrossentropy` loss function and `tf.metrics.BinaryAccuracy` metric.\n",
    "\n",
    "The sample uses the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), the same schedule as BERT pre-training is used: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5).\n",
    "\n",
    "#### Input pipelines\n",
    "\n",
    "The training code utilizes `tf.data` to implement input pipelines. The common techniques for optimizing performance - caching, prefetching - are applied. To better support distributed training the script allows for explicit configuration of [Auto Sharding Policy](https://www.tensorflow.org/tutorials/distribute/input).\n",
    "\n",
    "#### Fault tolerance\n",
    "\n",
    "The script utilizes the [`tf.keras.callbacks.experimental.BackupAndRestore`](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback) callback for resilience from failures during training. The callback provides fault tolerance, by backing up the model and current epoch number in a temporary checkpoint file. This is done at the endo of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "R4yEjEt8xa36"
   },
   "outputs": [],
   "source": [
    "! rm -rf scripts/trainer\n",
    "! mkdir -p scripts/trainer\n",
    "! touch scripts/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/trainer/task.py\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from official.nlp import optimization \n",
    "\n",
    "\n",
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "LOCAL_TB_FOLDER = '/tmp/logs'\n",
    "LOCAL_SAVED_MODEL_DIR = '/tmp/saved_model'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('steps_per_epoch', 625, 'Steps per training epoch')\n",
    "flags.DEFINE_integer('eval_steps', 150, 'Evaluation steps')\n",
    "flags.DEFINE_integer('epochs', 2, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 32, 'Per replica batch size')\n",
    "flags.DEFINE_string('training_data_path', f'/bert-finetuning/imdb/tfrecords/train', 'Training data GCS path')\n",
    "flags.DEFINE_string('validation_data_path', f'/bert-finetuning/imdb/tfrecords/valid', 'Validation data GCS path')\n",
    "flags.DEFINE_string('testing_data_path', f'/bert-finetuning/imdb/tfrecords/test', 'Testing data GCS path')\n",
    "\n",
    "flags.DEFINE_string('job_dir', f'/jobs', 'A base GCS path for jobs')\n",
    "flags.DEFINE_enum('strategy', 'multiworker', ['mirrored', 'multiworker'], 'Distribution strategy')\n",
    "flags.DEFINE_enum('auto_shard_policy', 'auto', ['auto', 'data', 'file', 'off'], 'Dataset sharing strategy')\n",
    "\n",
    "\n",
    "\n",
    "auto_shard_policy = {\n",
    "    'auto': tf.data.experimental.AutoShardPolicy.AUTO,\n",
    "    'data': tf.data.experimental.AutoShardPolicy.DATA,\n",
    "    'file': tf.data.experimental.AutoShardPolicy.FILE,\n",
    "    'off': tf.data.experimental.AutoShardPolicy.OFF,\n",
    "}\n",
    "\n",
    "\n",
    "def create_unbatched_dataset(tfrecords_folder):\n",
    "    \"\"\"Creates an unbatched dataset in the format required by the \n",
    "       sentiment analysis model from the folder with TFrecords files.\"\"\"\n",
    "    \n",
    "    feature_description = {\n",
    "        'text_fragment': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        return parsed_example['text_fragment'], parsed_example['label']\n",
    "  \n",
    "    file_paths = [f'{tfrecords_folder}/{file_path}' for file_path in tf.io.gfile.listdir(tfrecords_folder)]\n",
    "    dataset = tf.data.TFRecordDataset(file_paths)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def configure_dataset(ds, auto_shard_policy):\n",
    "    \"\"\"\n",
    "    Optimizes the performance of a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        auto_shard_policy\n",
    "    )\n",
    "    \n",
    "    ds = ds.repeat(-1).cache()\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds = ds.with_options(options)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_input_pipelines(train_dir, valid_dir, test_dir, batch_size, auto_shard_policy):\n",
    "    \"\"\"Creates input pipelines from Imdb dataset.\"\"\"\n",
    "    \n",
    "    train_ds = create_unbatched_dataset(train_dir)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = configure_dataset(train_ds, auto_shard_policy)\n",
    "    \n",
    "    valid_ds = create_unbatched_dataset(valid_dir)\n",
    "    valid_ds = valid_ds.batch(batch_size)\n",
    "    valid_ds = configure_dataset(valid_ds, auto_shard_policy)\n",
    "    \n",
    "    test_ds = create_unbatched_dataset(test_dir)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    test_ds = configure_dataset(test_ds, auto_shard_policy)\n",
    "\n",
    "    return train_ds, valid_ds, test_ds\n",
    "\n",
    "\n",
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder):\n",
    "    \"\"\"Builds a simple binary classification model with BERT trunk.\"\"\"\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "def copy_tensorboard_logs(local_path: str, gcs_path: str):\n",
    "    \"\"\"Copies Tensorboard logs from a local dir to a GCS location.\n",
    "    \n",
    "    After training, batch copy Tensorboard logs locally to a GCS location. This can result\n",
    "    in faster pipeline runtimes over streaming logs per batch to GCS that can get bottlenecked\n",
    "    when streaming large volumes.\n",
    "    \n",
    "    Args:\n",
    "      local_path: local filesystem directory uri.\n",
    "      gcs_path: cloud filesystem directory uri.\n",
    "    Returns:\n",
    "      None.\n",
    "    \"\"\"\n",
    "    pattern = '{}/*/events.out.tfevents.*'.format(local_path)\n",
    "    local_files = tf.io.gfile.glob(pattern)\n",
    "    gcs_log_files = [local_file.replace(local_path, gcs_path) for local_file in local_files]\n",
    "    for local_file, gcs_file in zip(local_files, gcs_log_files):\n",
    "        tf.io.gfile.copy(local_file, gcs_file)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    def _is_chief(task_type, task_id):\n",
    "        return ((task_type == 'chief' or task_type == 'worker') and task_id == 0) or task_type is None\n",
    "        \n",
    "    \n",
    "    logging.info('Setting up training.')\n",
    "    logging.info('   epochs: {}'.format(FLAGS.epochs))\n",
    "    logging.info('   steps_per_epoch: {}'.format(FLAGS.steps_per_epoch))\n",
    "    logging.info('   eval_steps: {}'.format(FLAGS.eval_steps))\n",
    "    logging.info('   strategy: {}'.format(FLAGS.strategy))\n",
    "    \n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_FOLDER)\n",
    "    \n",
    "    if FLAGS.strategy == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        \n",
    "    if strategy.cluster_resolver:    \n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id =(None, None)\n",
    "        \n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    \n",
    "    train_ds, valid_ds, test_ds = create_input_pipelines(\n",
    "        FLAGS.training_data_path,\n",
    "        FLAGS.validation_data_path,\n",
    "        FLAGS.testing_data_path,\n",
    "        global_batch_size,\n",
    "        auto_shard_policy[FLAGS.auto_shard_policy])\n",
    "        \n",
    "    num_train_steps = FLAGS.steps_per_epoch * FLAGS.epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    init_lr = 3e-5\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = build_classifier_model(TFHUB_HANDLE_PREPROCESS, TFHUB_HANDLE_ENCODER)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        metrics = tf.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=init_lr,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type='adamw')\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=loss,\n",
    "                      metrics=metrics)\n",
    "        \n",
    "    # Configure BackupAndRestore callback\n",
    "    backup_dir = '{}/backupandrestore'.format(FLAGS.job_dir)\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir)]\n",
    "    \n",
    "    # Configure TensorBoard callback on Chief\n",
    "    if _is_chief(task_type, task_id):\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=valid_ds,\n",
    "                        steps_per_epoch=FLAGS.steps_per_epoch,\n",
    "                        validation_steps=FLAGS.eval_steps,\n",
    "                        epochs=FLAGS.epochs,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    if _is_chief(task_type, task_id):\n",
    "        # Copy tensorboard logs to GCS\n",
    "        # tb_logs = '{}/tb_logs'.format(FLAGS.job_dir)\n",
    "        # logging.info('Copying TensorBoard logs to: {}'.format(tb_logs))\n",
    "        # copy_tensorboard_logs(LOCAL_TB_FOLDER, tb_logs)\n",
    "        saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    else:\n",
    "        saved_model_dir = LOCAL_SAVED_MODEL_DIR\n",
    "        \n",
    "    # Save trained model\n",
    "    saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(saved_model_dir))\n",
    "    model.save(saved_model_dir)\n",
    "    #tf.saved_model.save(model, saved_model_dir)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/imdb_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "RUN pip install pip install tf-models-official==2.4.0 tensorflow-text==2.4.1\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "'''\n",
    "\n",
    "with open('scripts/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a container image and upload it to your Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  23.04kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-4\n",
      " ---> ab93ebea3c35\n",
      "Step 2/5 : RUN pip install pip install tf-models-official==2.4.0 tensorflow-text==2.4.1\n",
      " ---> Using cache\n",
      " ---> ea1dfba482f6\n",
      "Step 3/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 39b395c7948b\n",
      "Step 4/5 : COPY trainer /trainer\n",
      " ---> 297e42bb7e7a\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in d217e98caa6c\n",
      "Removing intermediate container d217e98caa6c\n",
      " ---> 9ce947a458f1\n",
      "Successfully built 9ce947a458f1\n",
      "Successfully tagged gcr.io/rt-vertex-sandbox/imdb_bert:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/rt-vertex-sandbox/imdb_bert]\n",
      "\n",
      "\u001b[1Bc006d30d: Preparing \n",
      "\u001b[1Bdc34e21d: Preparing \n",
      "\u001b[1B7b2f60d1: Preparing \n",
      "\u001b[1Beb2eb480: Preparing \n",
      "\u001b[1Bc3eb7ae8: Preparing \n",
      "\u001b[1B1f0fa8db: Preparing \n",
      "\u001b[1B1854aed3: Preparing \n",
      "\u001b[1B3f816411: Preparing \n",
      "\u001b[1Bbf49b163: Preparing \n",
      "\u001b[1Bae69fc5d: Preparing \n",
      "\u001b[1B7465dde9: Preparing \n",
      "\u001b[1B3d1ada9a: Preparing \n",
      "\u001b[1Bf07e787b: Preparing \n",
      "\u001b[1Bbe96190a: Preparing \n",
      "\u001b[1B4bbff46e: Preparing \n",
      "\u001b[1B9cefae00: Preparing \n",
      "\u001b[1B0ad88149: Preparing \n",
      "\u001b[1B3ab21099: Preparing \n",
      "\u001b[1B09736a4b: Preparing \n",
      "\u001b[1Be844d06f: Preparing \n",
      "\u001b[1B54c6ced7: Preparing \n",
      "\u001b[1B34b5cf74: Preparing \n",
      "\u001b[1B0a9a6a11: Preparing \n",
      "\u001b[16Bf49b163: Waiting g \n",
      "\u001b[1B8f196cf4: Preparing \n",
      "\u001b[17Be69fc5d: Waiting g \n",
      "\u001b[17B465dde9: Waiting g \n",
      "\u001b[1Ba966f459: Preparing \n",
      "\u001b[1Bb9e63cdf: Preparing \n",
      "\u001b[1B49f5bf51: Preparing \n",
      "\u001b[19B07e787b: Waiting g \n",
      "\u001b[18Bbbff46e: Waiting g \n",
      "\u001b[1Bdd81f9fa: Preparing \n",
      "\u001b[34B006d30d: Pushed lready exists \u001b[30A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[34A\u001b[2Klatest: digest: sha256:ee47f2cfba1e972323fa92b2476a4d7b7a5cafdd54c98b3d9eb0c9f6707e8a87 size: 7461\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you could use Cloud Build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!gcloud builds submit --tag {TRAIN_IMAGE} .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the image locally\n",
    "\n",
    "It may be difficult to troubleshoot distributed training jobs running in AI Platform. You can perform some level of troubleshooting by simulating a distributed training environment on your AI Platform Notebooks instance.\n",
    "\n",
    "Let's assume that you have provisioned your instance with 4 GPUs. To simulate a distributed environment with two nodes, each equipped with two GPUs you can start two local containers configured as per below sample commands. Execute these commands from Jupyter terminal windows.\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=0,1\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }' \\\n",
    "--network=host \\\n",
    "gcr.io/{PROJECT}/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir={STAGING_BUCKET}/test_run\n",
    "```\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=2,3\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 1} }' \\\n",
    "--network=host \\\n",
    "gcr.io/{PROJECT}/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir={STAGING_BUCKET}/test_run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting training jobs\n",
    "\n",
    "The AI Platform (Unified) SDK works as a client/server model. On your side, the Python script, you  create a client that sends requests and receives responses from the server -- AI Platform. Requests and responses conform to the schemas documented in [AI Platform API Reference](https://cloud.google.com/ai-platform-unified/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create).\n",
    "\n",
    "We will use the term specification to refer to a formatted request. To submit a Custom job request you need to create a Custom job specification.\n",
    "\n",
    "The custom job specification comprises two parts:\n",
    "- A worker pool configuration, and\n",
    "- A scheduling configuration\n",
    "\n",
    "For single-node training, you define a single worker pool. For multi-node distributed training, multiple worker pools are defined.\n",
    "\n",
    "Within a worker pool specification, you configure:\n",
    "- Machine types and accelerators\n",
    "- Configuration of what training code the worker pool runs. \n",
    "\n",
    "For jobs using custom containers (like in this sample), the latter section of a worker pool specification contains a custom container configuration, including the URI of the container image and parameters passed to the container.\n",
    "\n",
    "The scheduling configuration includes parameters related to queuing and scheduling of custom jobs, including the maximum job running time and the job restart policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for single-node training\n",
    "\n",
    "For this job we will use a single `n1-standard-4` machine with 2 NVidia V100 GPUs and the container image created in the previous sections of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_V100', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When configuring a custom container you pass the command line parameters expected by your script through the `args` field of the container specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "per_replica_batch_size = 32\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = f'job-{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'.format()\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--testing_data_path=\" + testing_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--strategy=mirrored\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{job_id}',\n",
    "    worker_pool_specs=worker_pool_spec,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_id}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting the job\n",
    "\n",
    "To submit the job you need invoke the `custom_job.run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/437422844431/locations/us-central1/customJobs/2133971749606260736\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/437422844431/locations/us-central1/customJobs/2133971749606260736')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2133971749606260736?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+437422844431+locations+us-central1+tensorboards+6509988445736140800+experiments+2133971749606260736\n"
     ]
    }
   ],
   "source": [
    "custom_job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring the job\n",
    "\n",
    "You can monitor the job through GCP Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-job-20210622205137\n",
      "Job Resource Name: projects/437422844431/locations/us-central1/customJobs/2133971749606260736\n",
      "\n",
      "Check training progress at https://console.cloud.google.com/ai/platform/locations/us-central1/training/2133971749606260736?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/2133971749606260736 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/437422844431/locations/us-central1/customJobs/2133971749606260736\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {custom_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for multi-worker training\n",
    "\n",
    "If you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica.\n",
    "\n",
    "Each replica in the training cluster is given a single role or task in distributed training. For example:\n",
    "\n",
    "- Primary replica: Exactly one replica is designated the primary replica. This task manages the others and reports status for the job as a whole.\n",
    "- Worker(s): One or more replicas may be designated as workers. These replicas do their portion of the work as you designate in your job configuration.\n",
    "- Parameter server(s): If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters and coordinate shared model state between the workers.\n",
    "- Evaluator(s): If supported by your ML framework, one or more replicas may be designated as evaluators. These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n",
    "\n",
    "You configure the role by mapping to a worker pool specification:\n",
    "\n",
    "- First worker pool specification (index 0 in the `workerPoolSpecs` list) maps to Primary or chief worker. There can be only one replica configured in the first worker pool specification\n",
    "- Second worker pool specification maps to secondary workers\n",
    "- Third worker pool specification maps to parameters servers, and\n",
    "- Fourth worker pool specification maps to evaluators\n",
    "\n",
    "Our second job will be a multi-worker distributed training job with one chief and one secondary worker. Both replicas will run on `n1-standard-4` machines with two NVidia V100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = ('NVIDIA_TESLA_V100', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "per_replica_batch_size = 32\n",
    "training_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/train'\n",
    "validation_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/valid'\n",
    "testing_data_path = f'{STAGING_BUCKET}/bert-finetuning/imdb/tfrecords/test'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = f'{STAGING_BUCKET}/jobs/{job_id}'\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--testing_data_path=\" + testing_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--testing_data_path=\" + testing_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_m_job = vertex_ai.CustomJob(\n",
    "    display_name=f'imdb-bert-{job_id}',\n",
    "    worker_pool_specs=worker_pool_spec,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_id}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting the job\n",
    "\n",
    "To submit the job you need invoke the `custom_job.run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/437422844431/locations/us-central1/customJobs/5885470239205883904\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/437422844431/locations/us-central1/customJobs/5885470239205883904')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5885470239205883904?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+437422844431+locations+us-central1+tensorboards+6509988445736140800+experiments+5885470239205883904\n"
     ]
    }
   ],
   "source": [
    "custom_m_job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring the job\n",
    "\n",
    "You can monitor the job through GCP Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: imdb-bert-job-20210622221810\n",
      "Job Resource Name: projects/437422844431/locations/us-central1/customJobs/5885470239205883904\n",
      "\n",
      "Check training progress at https://console.cloud.google.com/ai/platform/locations/us-central1/training/5885470239205883904?project=437422844431\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/437422844431/locations/us-central1/customJobs/5885470239205883904 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job Name: {custom_m_job.display_name}\")\n",
    "print(f\"Job Resource Name: {custom_m_job.resource_name}\\n\")\n",
    "print(f\"Check training progress at {custom_m_job._dashboard_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ucaip_customjob_image_container.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-cpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
